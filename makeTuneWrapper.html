<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Fuse learner with tuning.</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="R.css">

<link rel="stylesheet" href="http://yandex.st/highlightjs/7.3/styles/github.min.css">
<script src="http://yandex.st/highlightjs/7.3/highlight.min.js"></script>
<script src="http://yandex.st/highlightjs/7.3/languages/r.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</head><body>

<table width="100%" summary="page for makeTuneWrapper {mlr}"><tr><td>makeTuneWrapper {mlr}</td><td align="right">R Documentation</td></tr></table>

<h2>Fuse learner with tuning.</h2>

<h3>Description</h3>

<p>Fuses a base learner with a search strategy to select its
hyperparameters. Creates a learner object, which can be
used like any other learner object, but which internally
uses tune. If the train function is called on it, the
search strategy and resampling are invoked to select an
optimal set of hyperparameter values. Finally, a model is
fitted on the complete training data with these optimal
hyperparameters and returned. See
<code><a href="tuneParams.html">tuneParams</a></code> for more details.
</p>


<h3>Usage</h3>

<pre>
  makeTuneWrapper(learner, resampling, measures, par.set,
    control, show.info = TRUE)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>learner</code></td>
<td>
<p>[<code><a href="Learner.html">Learner</a></code> or string]<br>
Learning algorithm.</p>
</td></tr>
<tr valign="top"><td><code>resampling</code></td>
<td>
<p>[<code><a href="ResampleDesc.html">ResampleDesc</a></code> |
<code><a href="ResampleInstance.html">ResampleInstance</a></code>]<br> Resampling
strategy to evaluate points in hyperparameter space.</p>
</td></tr>
<tr valign="top"><td><code>measures</code></td>
<td>
<p>[list of <code><a href="Measure.html">Measure</a></code>]<br>
Performance measures to evaluate. The first measure,
aggregated by the first aggregation function is optimized
during tuning, others are simply evaluated.</p>
</td></tr>
<tr valign="top"><td><code>par.set</code></td>
<td>
<p>[<code><a href="../../ParamHelpers/html/ParamSet.html">ParamSet</a></code>] <br>
Collection of parameters and their constraints for
optimization.</p>
</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>
<p>[<code><a href="TuneControl.html">TuneControl</a></code>] <br> Control
object for search method. Also selects the optimization
algorithm for tuning.</p>
</td></tr>
<tr valign="top"><td><code>show.info</code></td>
<td>
<p>[<code>logical(1)</code>]<br> Show info message
after each hyperparameter evaluation?  Default is
<code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After training, the optimal hyperparameters (and other
related information) can be retrieved with
<code><a href="getTuneResult.html">getTuneResult</a></code>.
</p>


<h3>Value</h3>

<p>[<code><a href="makeLearner.html">Learner</a></code>].
</p>


<h3>Examples</h3>

<pre><code class="r">
task = makeClassifTask(data=iris, target=&quot;Species&quot;)
lrn = makeLearner(&quot;classif.ksvm&quot;)
</code></pre>

<pre><code>## Lade n√∂tiges Paket: kernlab
</code></pre>

<pre><code class="r"># stupid mini grid
ps = makeParamSet(
  makeDiscreteParam(&quot;C&quot;, values = 1:2),
  makeDiscreteParam(&quot;sigma&quot;, values = 1:2)
)
ctrl = makeTuneControlGrid()
inner = makeResampleDesc(&quot;Holdout&quot;)
outer = makeResampleDesc(&quot;CV&quot;, iters = 2)
lrn = makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl)
mod = train(lrn, task)
</code></pre>

<pre><code>## [Tune] Started tuning learner classif.ksvm for parameter set:
## Disc param &#39;C&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## Disc param &#39;sigma&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## With control class: TuneControlGrid
## [Tune] 1: C=1,sigma=1 : mmce.test.mean=0.02
## [Tune] 1: C=2,sigma=1 : mmce.test.mean=0.02
## [Tune] 1: C=1,sigma=2 : mmce.test.mean=0.02
## [Tune] 1: C=2,sigma=2 : mmce.test.mean=0.04
## [Tune] Result: C=1,sigma=2 : =0.02
</code></pre>

<pre><code class="r">print(getTuneResult(mod))
</code></pre>

<pre><code>## Tune result:
## Op. pars: C=1,sigma=2
## =0.02
</code></pre>

<pre><code class="r"># nested resampling for evaluation
# we also extract tuned hyper pars in each iteration
r = resample(lrn, task, outer, extract = function(model) {
  getTuneResult(model)$x
})
</code></pre>

<pre><code>## [Resample] cross-validation iter: 1
## [Tune] Started tuning learner classif.ksvm for parameter set:
## Disc param &#39;C&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## Disc param &#39;sigma&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## With control class: TuneControlGrid
## [Tune] 1: C=1,sigma=1 : mmce.test.mean=0.04
## [Tune] 1: C=2,sigma=1 : mmce.test.mean=0.04
## [Tune] 1: C=1,sigma=2 : mmce.test.mean=0.04
## [Tune] 1: C=2,sigma=2 : mmce.test.mean=0.04
## [Tune] Result: C=2,sigma=2 : =0.04
## [Resample] cross-validation iter: 2
## [Tune] Started tuning learner classif.ksvm for parameter set:
## Disc param &#39;C&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## Disc param &#39;sigma&#39;. Vals: 1,2. Trafo: FALSE. Requires: FALSE
## With control class: TuneControlGrid
## [Tune] 1: C=1,sigma=1 : mmce.test.mean=   0
## [Tune] 1: C=2,sigma=1 : mmce.test.mean=   0
## [Tune] 1: C=1,sigma=2 : mmce.test.mean=   0
## [Tune] 1: C=2,sigma=2 : mmce.test.mean=   0
## [Tune] Result: C=1,sigma=2 : =   0
## [Resample] Result: mmce.test.mean=0.0533
</code></pre>

<pre><code class="r">print(r$extract)
</code></pre>

<pre><code>## [[1]]
## [[1]]$C
## [1] 2
## 
## [[1]]$sigma
## [1] 2
## 
## 
## [[2]]
## [[2]]$C
## [1] 1
## 
## [[2]]$sigma
## [1] 2
</code></pre>

<pre><code class="r">
</code></pre>


<hr><div align="center">[Package <em>mlr</em> version 1.2 <a href="00Index.html">Index</a>]</div>
</body></html>
