Feature Selection
==================

Often, data sets include a large number of features and it is desirable to reduce them. 
The technique of selecting a subset of relevant features is called feature selection. 
Feature selection can make the model better interpretable, the learning process
faster, and the fitted model more general by removing irrelevant features.
Different approaches exist, in order to figure out, which the relevant features
are. **mlr** supports [filters](#Filter) and [wrappers](#Wrapper).

Filters
-------

Filters are the simplest approach to find features that do not contain a lot of
additional information given the other features and can be left out.
Different methods are built into **mlr**'s function \man[getFeatureFilterValues]
and all access filter algorithms from the package `FSelector`.
The function is given a \man[Task] and simply returns a vector characterising
the importance of the different features.

```{r}
library("mlr")
task = makeClassifTask(data=iris, target="Species")
importance = getFilterValues(task, method="information.gain")
importance
```

According to this filter, `Petal.Width` and `Petal.Length` contain the most information.
With **mlr**'s function \man[filterFeatures], you can now filter the features in
the task by leaving out all but a given number of features with the highest
feature importance.

```{r}
filtered.task = filterFeatures(task, method="information.gain", select="abs", val=2)
```

Other filter options in \man[filterFeatures] are to require a percentage of
features to be filtered instead or to set a threshold for the numerical importance
values.

In a proper experimental set up you might want to automate the selection of the
features so that it can be part of the validation method of your choice.
We will use the standard 10-fold cross validation.

```{r}
learner = makeLearner("classif.fnn")
learnerFiltered = makeFilterWrapper(learner=learner, fw.method="information.gain", fw.select="perc", fw.val=0.7)
rdesc = makeResampleDesc("CV", iters=10) 
rsres = resample(learner=learnerFiltered, task=task, resampling=rdesc, show.info=FALSE, models=TRUE)
rsres$aggr
```

You may want to know which features have been used. Luckily, we have called
`resample` with the argument `models=TRUE`, which means that `rsres$models`
contains a `list` of each model used for a fold.
In this case the \man[Learner] is also of the class \man[FilterWrapper] and we can call
\man[getFilteredFeatures] on each model.

```{r}
sfeats = sapply(rsres$models, getFilteredFeatures)
table(sfeats)
```

The selection of features seems to be very stable.
The `Sepal.Width` did not make it into a single fold.

### Tuning the threshold
```{r}
library("mlbench")
data(Sonar)
task = makeClassifTask(data = Sonar, target = "Class", positive = "M")
lrn = makeLearner("classif.rpart")
lrnFiltered = makeFilterWrapper(learner=lrn, fw.method="chi.squared", fw.select="threshold", fw.val=0)
ps = makeParamSet(makeDiscreteParam("fw.val", values = seq(from=0.2, 0.4, by=0.05)))
tuneRes = tuneParams(lrnFiltered, task=task, resampling=makeResampleDesc("CV", iters=5), par.set=ps, control=makeTuneControlGrid())
```

Wrapper
-------

Unlike the filters, wrappers use the performance a learner can
achieve on a given subset of the features in the data to do the filtering.

#### Classification example

Let's train a decision tree on the ``iris`` data and use a sequential forward
search to find the best set of features w.r.t. the mean misclassification error
(\man[mmce]).

```{r}
library("mlr")
task = makeClassifTask(data=iris, target="Species")
lrn = makeLearner("classif.rpart")
rdesc = makeResampleDesc("Holdout") 

ctrlSeq = makeFeatSelControlSequential(method="sfs")
sfSeq = selectFeatures(learner=lrn, task=task, resampling=rdesc, control=ctrlSeq)
sfSeq
analyzeFeatSelResult(sfSeq, reduce=FALSE)
``` 


#### Regression example

We fit a simple linear regression model to the ``BostonHousing`` data set and
use a genetic algorithm to find a feature set that minimises the mean squared
error (\man[mse]).

```{r}
library("mlbench")
data(BostonHousing)

task = makeRegrTask(data = BostonHousing, target = "medv")
lrn = makeLearner("regr.lm")
rdesc = makeResampleDesc("Holdout") 

ctrlGA = makeFeatSelControlGA(maxit=10)
sfGA = selectFeatures(learner=lrn, task=task, resampling=rdesc, control=ctrlGA, show.info=FALSE)
sfGA
```

