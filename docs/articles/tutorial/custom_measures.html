<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Integrating Another Measure â€¢ mlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../index.html">mlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/mlr.html">Get Started</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/task.html">Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learners.html">Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/predict.html">Predict</a>
    </li>
    <li>
      <a href="../../articles/tutorial/performance.html">Performance</a>
    </li>
    <li>
      <a href="../../articles/tutorial/resample.html">Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/tune.html">Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/benchmark_experiments.html">Benchmark Experiments</a>
    </li>
    <li>
      <a href="../../articles/tutorial/parallelization.html">Parallelization</a>
    </li>
    <li>
      <a href="../../articles/tutorial/visualization.html">Visualization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/configuring.html">Configuration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/wrapper.html">Wrapped Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/preproc.html">Preprocessing</a>
    </li>
    <li>
      <a href="../../articles/tutorial/impute.html">Imputation</a>
    </li>
    <li>
      <a href="../../articles/tutorial/bagging.html">Generic Bagging</a>
    </li>
    <li>
      <a href="../../articles/tutorial/advanced_tune.html">Advanced Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/feature_selection.html">Feature Selection</a>
    </li>
    <li>
      <a href="../../articles/tutorial/nested_resampling.html">Nested Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/cost_sensitive_classif.html">Cost-Sensitive Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/imbalanced_classification_problems.html">Imbalanced Classification Problems</a>
    </li>
    <li>
      <a href="../../articles/tutorial/roc_analysis.html">ROC Analysis and Performance Curves</a>
    </li>
    <li>
      <a href="../../articles/tutorial/multilabel.html">Multilabel Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learning_curve_analysis.html">Learning Curve Analysis</a>
    </li>
    <li>
      <a href="../../articles/tutorial/partial_dependence.html">Partial Dependence Plots</a>
    </li>
    <li>
      <a href="../../articles/tutorial/classifier_calibration.html">Classifier Calibration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/hyperparameter_tuning_effects.html">Hyperparameter Tuning Effects</a>
    </li>
    <li>
      <a href="../../articles/tutorial/out_of_bag_predictions.html">Out-of-Bag Predictions</a>
    </li>
    <li>
      <a href="../../articles/tutorial/handling_of_spatial_data.html">Handling of Spatial Data</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Extend
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/custom_learners.html">Create Custom Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/custom_measures.html">Create Custom Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_imputation.html">Create Imputation Methods</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_filter.html">Create Custom Filters</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Appendix
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/example_tasks.html">Example Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/integrated_learners.html">Integrated Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/implemented_measures.html">Implemented Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/filter_methods.html">Integrated Filter Methods</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlr-org/mlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form" role="search" method="GET" action="../../search.html">
        <div class="input-group">
          <input type="text" class="form-control" placeholder="Search" id="search" name="search">
</div>
        <button type="submit" class="btn btn-default">
          <i class="glyphicon glyphicon-search"></i>
        </button>
      </form>
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Integrating Another Measure</h1>
            
          </div>

    
    
<div class="contents">
<p>In some cases, you might want to evaluate a [&amp;Prediction] or [&amp;ResamplePrediction] with a <a href="&amp;makeMeasure">Measure</a> which is not yet implemented in [%mlr]. This could be either a performance measure which is not listed in the <a href="measures.md">Appendix</a> or a measure that uses a misclassification cost matrix.</p>
<div id="performance-measures-and-aggregation-schemes" class="section level2">
<h2 class="hasAnchor">
<a href="#performance-measures-and-aggregation-schemes" class="anchor"></a>Performance measures and aggregation schemes</h2>
<p>Performance measures in [%mlr] are objects of class <a href="&amp;makeMeasure">Measure</a>. For example the <a href="measures.md">mse</a> (mean squared error) looks as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(mse)</code></pre></div>
<pre><code>## List of 10
##  $ id        : chr "mse"
##  $ minimize  : logi TRUE
##  $ properties: chr [1:3] "regr" "req.pred" "req.truth"
##  $ fun       :function (task, model, pred, feats, extra.args)  
##  $ extra.args: list()
##  $ best      : num 0
##  $ worst     : num Inf
##  $ name      : chr "Mean of squared errors"
##  $ note      : chr "Defined as: mean((response - truth)^2)"
##  $ aggr      :List of 4
##   ..$ id        : chr "test.mean"
##   ..$ name      : chr "Test mean"
##   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  
##   ..$ properties: chr "req.test"
##   ..- attr(*, "class")= chr "Aggregation"
##  - attr(*, "class")= chr "Measure"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse$fun</code></pre></div>
<pre><code>## function (task, model, pred, feats, extra.args) 
## {
##     measureMSE(pred$data$truth, pred$data$response)
## }
## &lt;bytecode: 0x7897fc8&gt;
## &lt;environment: namespace:mlr&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">measureMSE</code></pre></div>
<pre><code>## function (truth, response) 
## {
##     mean((response - truth)^2)
## }
## &lt;bytecode: 0x5e52e80&gt;
## &lt;environment: namespace:mlr&gt;</code></pre>
<p>See the <a href="&amp;makeMeasure">Measure</a> documentation page for a detailed description of the object slots.</p>
<p>At the core is slot <code>$fun</code> which contains the function that calculates the performance value. The actual work is done by function <a href="&amp;measures">measureMSE</a>. Similar functions, generally adhering to the naming scheme <code>measure</code> followed by the capitalized measure ID, exist for most performance measures. See the [&amp;measures] help page for a complete list.</p>
<p>Just as [&amp;Task] and <a href="&amp;makeLearner">Learner</a> objects each <a href="&amp;makeMeasure">Measure</a> has an identifier <code>$id</code> which is for example used to annotate results and plots. For plots there is also the option to use the longer measure <code>$name</code> instead. See the tutorial page on <a href="visualization.md">Visualization</a> for more information.</p>
<p>Moreover, a <a href="&amp;makeMeasure">Measure</a> includes a number of <code>$properties</code> that indicate for which types of learning problems it is suitable and what information is required to calculate it. Obviously, most measures need the [&amp;Prediction] object (<code>"req.pred"</code>) and, for supervised problems, the true values of the target variable(s) (<code>"req.truth"</code>). You can use functions <a href="&amp;MeasureProperties">getMeasureProperties</a> and <a href="&amp;MeasureProperties">hasMeasureProperties</a> to determine the properties of a <a href="&amp;makeMeasure">Measure</a>. Moreover, [&amp;listMeasureProperties] shows all measure properties currently available in [%mlr].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/listMeasureProperties.html">listMeasureProperties</a></span>()</code></pre></div>
<pre><code>##  [1] "classif"       "classif.multi" "multilabel"    "regr"         
##  [5] "surv"          "cluster"       "costsens"      "req.pred"     
##  [9] "req.truth"     "req.task"      "req.feats"     "req.model"    
## [13] "req.prob"</code></pre>
<p>Additional to its properties, each <a href="&amp;makeMeasure">Measure</a> knows its extreme values <code>$best</code> and <code>$worst</code> and if it wants to be minimized or maximized (<code>$minimize</code>) during <a href="tune.md">tuning</a> or <a href="feature_selection.md">feature selection</a>.</p>
<p>For resampling slot <code>$aggr</code> specifies how the overall performance across all resampling iterations is calculated. Typically, this is just a matter of aggregating the performance values obtained on the test sets <code>perf.test</code> or the training sets <code>perf.train</code> by a simple function. The by far most common scheme is <a href="&amp;aggregations">test.mean</a>, i.e., the unweighted mean of the performances on the test sets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(test.mean)</code></pre></div>
<pre><code>## List of 4
##  $ id        : chr "test.mean"
##  $ name      : chr "Test mean"
##  $ fun       :function (task, perf.test, perf.train, measure, group, pred)  
##  $ properties: chr "req.test"
##  - attr(*, "class")= chr "Aggregation"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test.mean$fun</code></pre></div>
<pre><code>## function (task, perf.test, perf.train, measure, group, pred) 
## mean(perf.test)
## &lt;bytecode: 0x844d5e8&gt;
## &lt;environment: namespace:mlr&gt;</code></pre>
<p>All aggregation schemes are objects of class [&amp;Aggregation] with the function in slot <code>$fun</code> doing the actual work. The <code>$properties</code> member indicates if predictions (or performance values) on the training or test data sets are required to calculate the aggregation.</p>
<p>You can change the aggregation scheme of a <a href="&amp;makeMeasure">Measure</a> via function [&amp;setAggregation]. See the tutorial page on <a href="resample.md">resampling</a> for some examples and the [&amp;aggregations] help page for all available aggregation schemes.</p>
<p>You can construct your own <a href="&amp;makeMeasure">Measure</a> and [&amp;Aggregation] objects via functions [&amp;makeMeasure], [&amp;makeCostMeasure], [&amp;makeCustomResampledMeasure] and [&amp;makeAggregation]. Some examples are shown in the following.</p>
</div>
<div id="constructing-a-performance-measure" class="section level2">
<h2 class="hasAnchor">
<a href="#constructing-a-performance-measure" class="anchor"></a>Constructing a performance measure</h2>
<p>Function [&amp;makeMeasure] provides a simple way to construct your own performance measure.</p>
<p>Below this is exemplified by re-implementing the mean misclassification error (<a href="measures.md">mmce</a>). We first write a function that computes the measure on the basis of the true and predicted class labels. Note that this function must have certain formal arguments listed in the documentation of [&amp;makeMeasure]. Then the <a href="&amp;makeMeasure">Measure</a> object is created and we work with it as usual with the [&amp;performance] function.</p>
<p>See the <strong>R</strong> documentation of [&amp;makeMeasure] for more details on the various parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Define a function that calculates the misclassification rate
my.mmce.fun =<span class="st"> </span>function(task, model, pred, feats, extra.args) {
  tb =<span class="st"> </span><span class="kw">table</span>(<span class="kw"><a href="../../reference/getPredictionResponse.html">getPredictionResponse</a></span>(pred), <span class="kw"><a href="../../reference/getPredictionResponse.html">getPredictionTruth</a></span>(pred))
  <span class="dv">1</span> -<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(tb)) /<span class="st"> </span><span class="kw">sum</span>(tb)
}

## Generate the Measure object
my.mmce =<span class="st"> </span><span class="kw"><a href="../../reference/makeMeasure.html">makeMeasure</a></span>(
  <span class="dt">id =</span> <span class="st">"my.mmce"</span>, <span class="dt">name =</span> <span class="st">"My Mean Misclassification Error"</span>,
  <span class="dt">properties =</span> <span class="kw">c</span>(<span class="st">"classif"</span>, <span class="st">"classif.multi"</span>, <span class="st">"req.pred"</span>, <span class="st">"req.truth"</span>),
  <span class="dt">minimize =</span> <span class="ot">TRUE</span>, <span class="dt">best =</span> <span class="dv">0</span>, <span class="dt">worst =</span> <span class="dv">1</span>,
  <span class="dt">fun =</span> my.mmce.fun
)

## Train a learner and make predictions
mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(<span class="st">"classif.lda"</span>, iris.task)
pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">task =</span> iris.task)

## Calculate the performance using the new measure
<span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> my.mmce)</code></pre></div>
<pre><code>## my.mmce 
##    0.02</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Apparently the result coincides with the mlr implementation
<span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> mmce)</code></pre></div>
<pre><code>## mmce 
## 0.02</code></pre>
</div>
<div id="constructing-a-measure-for-ordinary-misclassification-costs" class="section level2">
<h2 class="hasAnchor">
<a href="#constructing-a-measure-for-ordinary-misclassification-costs" class="anchor"></a>Constructing a measure for ordinary misclassification costs</h2>
<p>For in depth explanations and details see the tutorial page on <a href="cost_sensitive_classif.md">cost-sensitive classification</a>.</p>
<p>To create a measure that involves ordinary, i.e., class-dependent misclassification costs you can use function [&amp;makeCostMeasure]. You first need to define the cost matrix. The rows indicate true and the columns predicted classes and the rows and columns have to be named by the class labels. The cost matrix can then be wrapped in a <a href="&amp;makeMeasure">Measure</a> object and predictions can be evaluated as usual with the [&amp;performance] function.</p>
<p>See the <strong>R</strong> documentation of function [&amp;makeCostMeasure] for details on the various parameters.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Create the cost matrix
costs =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">ncol =</span> <span class="dv">3</span>)
<span class="kw">rownames</span>(costs) =<span class="st"> </span><span class="kw">colnames</span>(costs) =<span class="st"> </span><span class="kw"><a href="../../reference/getTaskClassLevels.html">getTaskClassLevels</a></span>(iris.task)

## Encapsulate the cost matrix in a Measure object
my.costs =<span class="st"> </span><span class="kw"><a href="../../reference/makeCostMeasure.html">makeCostMeasure</a></span>(
  <span class="dt">id =</span> <span class="st">"my.costs"</span>, <span class="dt">name =</span> <span class="st">"My Costs"</span>,
  <span class="dt">costs =</span> costs,
  <span class="dt">minimize =</span> <span class="ot">TRUE</span>, <span class="dt">best =</span> <span class="dv">0</span>, <span class="dt">worst =</span> <span class="dv">3</span>
)

## Train a learner and make a prediction
mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(<span class="st">"classif.lda"</span>, iris.task)
pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> iris)

## Calculate the average costs
<span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> my.costs)</code></pre></div>
<pre><code>##   my.costs 
## 0.02666667</code></pre>
</div>
<div id="creating-an-aggregation-scheme" class="section level2">
<h2 class="hasAnchor">
<a href="#creating-an-aggregation-scheme" class="anchor"></a>Creating an aggregation scheme</h2>
<p>It is possible to create your own aggregation scheme using function [&amp;makeAggregation]. You need to specify an identifier <code>id</code>, the <code>properties</code>, and write a function that does the actual aggregation. Optionally, you can <code>name</code> your aggregation scheme.</p>
<p>Possible settings for <code>properties</code> are <code>"req.test"</code> and <code>"req.train"</code> if predictions on either the training or test sets are required, and the vector <code>c("req.train", "req.test")</code> if both are needed.</p>
<p>The aggregation function must have a certain signature detailed in the documentation of [&amp;makeAggregation]. Usually, you will only need the performance values on the test sets <code>perf.test</code> or the training sets <code>perf.train</code>. In rare cases, e.g., the [&amp;Prediction] object <code>pred</code> or information stored in the [&amp;Task] object might be required to obtain the aggregated performance. For an example have a look at the <a href="https://github.com/mlr-org/mlr/blob/master/R/aggregations.R#L211">definition</a> of function <a href="&amp;aggregations">test.join</a>.</p>
<div id="example-evaluating-the-range-of-measures" class="section level3">
<h3 class="hasAnchor">
<a href="#example-evaluating-the-range-of-measures" class="anchor"></a>Example: Evaluating the range of measures</h3>
<p>Letâ€™s say you are interested in the range of the performance values obtained on individual test sets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my.range.aggr =<span class="st"> </span><span class="kw"><a href="../../reference/makeAggregation.html">makeAggregation</a></span>(<span class="dt">id =</span> <span class="st">"test.range"</span>, <span class="dt">name =</span> <span class="st">"Test Range"</span>,
  <span class="dt">properties =</span> <span class="st">"req.test"</span>,
  <span class="dt">fun =</span> function (task, perf.test, perf.train, measure, group, pred)
    <span class="kw">diff</span>(<span class="kw">range</span>(perf.test))
)</code></pre></div>
<p><code>perf.train</code> and <code>perf.test</code> are both numerical vectors containing the performances on the train and test data sets. In most cases (unless you are using bootstrap as resampling strategy or have set <code>predict = "both"</code> in [&amp;makeResampleDesc]) the <code>perf.train</code> vector is empty.</p>
<p>Now we can run a feature selection based on the first measure in the provided <a href="&amp;base::list">list</a> and see how the other measures turn out.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## mmce with default aggregation scheme test.mean
ms1 =<span class="st"> </span>mmce

## mmce with new aggregation scheme my.range.aggr
ms2 =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(ms1, my.range.aggr)

## Minimum and maximum of the mmce over test sets
ms1min =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(ms1, test.min)
ms1max =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(ms1, test.max)

## Feature selection
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)
res =<span class="st"> </span><span class="kw"><a href="../../reference/selectFeatures.html">selectFeatures</a></span>(<span class="st">"classif.rpart"</span>, iris.task, rdesc, <span class="dt">measures =</span> <span class="kw">list</span>(ms1, ms2, ms1min, ms1max),
  <span class="dt">control =</span> <span class="kw"><a href="../../reference/FeatSelControl.html">makeFeatSelControlExhaustive</a></span>(), <span class="dt">show.info =</span> <span class="ot">FALSE</span>)

## Optimization path, i.e., performances for the 16 possible feature subsets
perf.data =<span class="st"> </span><span class="kw">as.data.frame</span>(res$opt.path)
<span class="kw">head</span>(perf.data[<span class="dv">1</span>:<span class="dv">8</span>])</code></pre></div>
<pre><code>##   Sepal.Length Sepal.Width Petal.Length Petal.Width mmce.test.mean
## 1            0           0            0           0     0.66666667
## 2            1           0            0           0     0.28666667
## 3            0           1            0           0     0.46666667
## 4            0           0            1           0     0.05333333
## 5            0           0            0           1     0.04000000
## 6            1           1            0           0     0.32000000
##   mmce.test.range mmce.test.min mmce.test.max
## 1            0.10          0.60          0.70
## 2            0.08          0.24          0.32
## 3            0.06          0.44          0.50
## 4            0.06          0.02          0.08
## 5            0.06          0.02          0.08
## 6            0.18          0.22          0.40</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pd =<span class="st"> </span><span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">0.005</span>, <span class="dt">height =</span> <span class="dv">0</span>)
p =<span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> mmce.test.range, <span class="dt">y =</span> mmce.test.mean, <span class="dt">ymax =</span> mmce.test.max, <span class="dt">ymin =</span> mmce.test.min,
  <span class="dt">color =</span> <span class="kw">as.factor</span>(Sepal.Width), <span class="dt">pch =</span> <span class="kw">as.factor</span>(Petal.Width)), <span class="dt">data =</span> perf.data) +
<span class="st">  </span><span class="kw">geom_pointrange</span>(<span class="dt">position =</span> pd) +
<span class="st">  </span><span class="kw">coord_flip</span>()
<span class="kw">print</span>(p)</code></pre></div>
<p><img src="custom_measures_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
<p>The plot shows the range versus the mean misclassification error. The value on the y-axis thus corresponds to the length of the error bars. (Note that the points and error bars are jittered in y-direction.)</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#performance-measures-and-aggregation-schemes">Performance measures and aggregation schemes</a></li>
      <li><a href="#constructing-a-performance-measure">Constructing a performance measure</a></li>
      <li><a href="#constructing-a-measure-for-ordinary-misclassification-costs">Constructing a measure for ordinary misclassification costs</a></li>
      <li><a href="#creating-an-aggregation-scheme">Creating an aggregation scheme</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio, Mason Gallo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
