<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Feature Selection • mlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../index.html">mlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/mlr.html">Get Started</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/task.html">Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learners.html">Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/predict.html">Predict</a>
    </li>
    <li>
      <a href="../../articles/tutorial/performance.html">Performance</a>
    </li>
    <li>
      <a href="../../articles/tutorial/resample.html">Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/tune.html">Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/benchmark_experiments.html">Benchmark Experiments</a>
    </li>
    <li>
      <a href="../../articles/tutorial/parallelization.html">Parallelization</a>
    </li>
    <li>
      <a href="../../articles/tutorial/visualization.html">Visualization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/configuring.html">Configuration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/wrapper.html">Wrapped Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/preproc.html">Preprocessing</a>
    </li>
    <li>
      <a href="../../articles/tutorial/impute.html">Imputation</a>
    </li>
    <li>
      <a href="../../articles/tutorial/bagging.html">Generic Bagging</a>
    </li>
    <li>
      <a href="../../articles/tutorial/advanced_tune.html">Advanced Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/feature_selection.html">Feature Selection</a>
    </li>
    <li>
      <a href="../../articles/tutorial/nested_resampling.html">Nested Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/cost_sensitive_classif.html">Cost-Sensitive Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/imbalanced_classification_problems.html">Imbalanced Classification Problems</a>
    </li>
    <li>
      <a href="../../articles/tutorial/roc_analysis.html">ROC Analysis and Performance Curves</a>
    </li>
    <li>
      <a href="../../articles/tutorial/multilabel.html">Multilabel Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learning_curve_analysis.html">Learning Curve Analysis</a>
    </li>
    <li>
      <a href="../../articles/tutorial/partial_dependence.html">Partial Dependence Plots</a>
    </li>
    <li>
      <a href="../../articles/tutorial/classifier_calibration.html">Classifier Calibration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/hyperparameter_tuning_effects.html">Hyperparameter Tuning Effects</a>
    </li>
    <li>
      <a href="../../articles/tutorial/out_of_bag_predictions.html">Out-of-Bag Predictions</a>
    </li>
    <li>
      <a href="../../articles/tutorial/handling_of_spatial_data.html">Handling of Spatial Data</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Extend
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/custom_learners.html">Create Custom Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/custom_measures.html">Create Custom Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_imputation.html">Create Imputation Methods</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_filter.html">Create Custom Filters</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Appendix
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/example_tasks.html">Example Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/integrated_learners.html">Integrated Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/implemented_measures.html">Implemented Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/filter_methods.html">Integrated Filter Methods</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlr-org/mlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form" role="search" method="GET" action="../../search.html">
        <div class="input-group">
          <input type="text" class="form-control" placeholder="Search" id="search" name="search">
</div>
        <button type="submit" class="btn btn-default">
          <i class="glyphicon glyphicon-search"></i>
        </button>
      </form>
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Feature Selection</h1>
            
          </div>

    
    
<div class="contents">
<p>Often, data sets include a large number of features. The technique of extracting a subset of relevant features is called feature selection. Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance. There exist different approaches to identify the relevant features. [%mlr] supports <em>filter</em> and <em>wrapper methods</em>.</p>
<div id="filter-methods" class="section level2">
<h2 class="hasAnchor">
<a href="#filter-methods" class="anchor"></a>Filter methods</h2>
<p>Filter methods assign an importance value to each feature. Based on these values the features can be ranked and a feature subset can be selected.</p>
<div id="calculating-the-feature-importance" class="section level3">
<h3 class="hasAnchor">
<a href="#calculating-the-feature-importance" class="anchor"></a>Calculating the feature importance</h3>
<p>Different methods for calculating the feature importance are built into [%mlr]’s function [&amp;generateFilterValuesData] ([&amp;getFilterValues] has been deprecated in favor of [&amp;generateFilterValuesData].). Currently, classification, regression and survival analysis tasks are supported. A table showing all available methods can be found <a href="filter_methods.md">here</a>.</p>
<p>Function [&amp;generateFilterValuesData] requires the [&amp;Task] and a character string specifying the filter method.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fv =<span class="st"> </span><span class="kw"><a href="../../reference/generateFilterValuesData.html">generateFilterValuesData</a></span>(iris.task, <span class="dt">method =</span> <span class="st">"information.gain"</span>)
fv</code></pre></div>
<pre><code>## FilterValues:
## Task: iris-example
##           name    type information.gain
## 1 Sepal.Length numeric        0.4521286
## 2  Sepal.Width numeric        0.2672750
## 3 Petal.Length numeric        0.9402853
## 4  Petal.Width numeric        0.9554360</code></pre>
<p><code>fv</code> is a [&amp;FilterValues] object and <code>fv$data</code> contains a <a href="&amp;base::data.frame">data.frame</a> that gives the importance values for all features. Optionally, a vector of filter methods can be passed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fv2 =<span class="st"> </span><span class="kw"><a href="../../reference/generateFilterValuesData.html">generateFilterValuesData</a></span>(iris.task, <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">"information.gain"</span>, <span class="st">"chi.squared"</span>))
fv2$data</code></pre></div>
<pre><code>##           name    type information.gain chi.squared
## 1 Sepal.Length numeric        0.4521286   0.6288067
## 2  Sepal.Width numeric        0.2672750   0.4922162
## 3 Petal.Length numeric        0.9402853   0.9346311
## 4  Petal.Width numeric        0.9554360   0.9432359</code></pre>
<p>A bar plot of importance values for the individual features can be obtained using function [&amp;plotFilterValues].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotFilterValues.html">plotFilterValues</a></span>(fv2)</code></pre></div>
<p><img src="feature_selection_files/figure-html/unnamed-chunk-4-1.png" width="672"></p>
<p>By default [&amp;plotFilterValues] will create facetted subplots if multiple filter methods are passed as input to [&amp;generateFilterValuesData].</p>
<p>There is also an experimental [%ggvis] plotting function, [&amp;plotFilterValuesGGVIS]. This takes the same arguments as [&amp;plotFilterValues] and produces a [%shiny] application that allows the interactive selection of the displayed filter method, the number of features selected, and the sorting method (e.g., ascending or descending).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotFilterValuesGGVIS.html">plotFilterValuesGGVIS</a></span>(fv2)</code></pre></div>
<p>According to the <code>"information.gain"</code> measure, <code>Petal.Width</code> and <code>Petal.Length</code> contain the most information about the target variable <code>Species</code>.</p>
</div>
<div id="selecting-a-feature-subset" class="section level3">
<h3 class="hasAnchor">
<a href="#selecting-a-feature-subset" class="anchor"></a>Selecting a feature subset</h3>
<p>With [%mlr]’s function [&amp;filterFeatures] you can create a new [&amp;Task] by leaving out features of lower importance.</p>
<p>There are several ways to select a feature subset based on feature importance values:</p>
<ul>
<li>Keep a certain <em>absolute number</em> (<code>abs</code>) of features with highest importance.</li>
<li>Keep a certain <em>percentage</em> (<code>perc</code>) of features with highest importance.</li>
<li>Keep all features whose importance exceeds a certain <em>threshold value</em> (<code>threshold</code>).</li>
</ul>
<p>Function [&amp;filterFeatures] supports these three methods as shown in the following example. Moreover, you can either specify the <code>method</code> for calculating the feature importance or you can use previously computed importance values via argument <code>fval</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Keep the 2 most important features
filtered.task =<span class="st"> </span><span class="kw"><a href="../../reference/filterFeatures.html">filterFeatures</a></span>(iris.task, <span class="dt">method =</span> <span class="st">"information.gain"</span>, <span class="dt">abs =</span> <span class="dv">2</span>)

## Keep the 25% most important features
filtered.task =<span class="st"> </span><span class="kw"><a href="../../reference/filterFeatures.html">filterFeatures</a></span>(iris.task, <span class="dt">fval =</span> fv, <span class="dt">perc =</span> <span class="fl">0.25</span>)

## Keep all features with importance greater than 0.5
filtered.task =<span class="st"> </span><span class="kw"><a href="../../reference/filterFeatures.html">filterFeatures</a></span>(iris.task, <span class="dt">fval =</span> fv, <span class="dt">threshold =</span> <span class="fl">0.5</span>)
filtered.task</code></pre></div>
<pre><code>## Supervised task: iris-example
## Type: classif
## Target: Species
## Observations: 150
## Features:
##    numerics     factors     ordered functionals 
##           2           0           0           0 
## Missings: FALSE
## Has weights: FALSE
## Has blocking: FALSE
## Is spatial: FALSE
## Classes: 3
##     setosa versicolor  virginica 
##         50         50         50 
## Positive class: NA</code></pre>
</div>
<div id="fuse-a-learner-with-a-filter-method" class="section level3">
<h3 class="hasAnchor">
<a href="#fuse-a-learner-with-a-filter-method" class="anchor"></a>Fuse a learner with a filter method</h3>
<p>Often feature selection based on a filter method is part of the data preprocessing and in a subsequent step a learning method is applied to the filtered data. In a proper experimental setup you might want to automate the selection of the features so that it can be part of the validation method of your choice. A <a href="&amp;makeLearner">Learner</a> can be fused with a filter method by function [&amp;makeFilterWrapper]. The resulting <a href="&amp;makeLearner">Learner</a> has the additional class attribute [&amp;FilterWrapper].</p>
<p>In the following example we calculate the 10-fold cross-validated error rate (<a href="measures.md">mmce</a>) of the <a href="&amp;FNN::fnn">k nearest neighbor classifier</a> with preceding feature selection on the <a href="&amp;datasets::iris">iris</a> data set. We use <code>"information.gain"</code> as importance measure and select the 2 features with highest importance. In each resampling iteration feature selection is carried out on the corresponding training data set before fitting the learner.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeFilterWrapper.html">makeFilterWrapper</a></span>(<span class="dt">learner =</span> <span class="st">"classif.fnn"</span>, <span class="dt">fw.method =</span> <span class="st">"information.gain"</span>, <span class="dt">fw.abs =</span> <span class="dv">2</span>)
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">10</span>)
r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="dt">learner =</span> lrn, <span class="dt">task =</span> iris.task, <span class="dt">resampling =</span> rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>, <span class="dt">models =</span> <span class="ot">TRUE</span>)
r$aggr</code></pre></div>
<pre><code>## mmce.test.mean 
##     0.05333333</code></pre>
<p>You may want to know which features have been used. Luckily, we have called [&amp;resample] with the argument <code>models = TRUE</code>, which means that <code>r$models</code> contains a <a href="&amp;base::list">list</a> of <a href="&amp;makeWrappedModel">models</a> fitted in the individual resampling iterations. In order to access the selected feature subsets we can call [&amp;getFilteredFeatures] on each model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sfeats =<span class="st"> </span><span class="kw">sapply</span>(r$models, getFilteredFeatures)
<span class="kw">table</span>(sfeats)</code></pre></div>
<pre><code>## sfeats
## Petal.Length  Petal.Width 
##           10           10</code></pre>
<p>The selection of features seems to be very stable. The features <code>Sepal.Length</code> and <code>Sepal.Width</code> did not make it into a single fold.</p>
</div>
<div id="tuning-the-size-of-the-feature-subset" class="section level3">
<h3 class="hasAnchor">
<a href="#tuning-the-size-of-the-feature-subset" class="anchor"></a>Tuning the size of the feature subset</h3>
<p>In the above examples the number/percentage of features to select or the threshold value have been arbitrarily chosen. If filtering is a preprocessing step before applying a learning method optimal values with regard to the learner performance can be found by <a href="tune.md">tuning</a>.</p>
<p>In the following regression example we consider the <a href="&amp;mlbench::BostonHousing">BostonHousing</a> data set. We use a <a href="&amp;stats::lm">linear regression model</a> and determine the optimal percentage value for feature selection such that the 3-fold cross-validated <a href="&amp;mse">mean squared error</a> of the learner is minimal. As search strategy for tuning a grid search is used.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeFilterWrapper.html">makeFilterWrapper</a></span>(<span class="dt">learner =</span> <span class="st">"regr.lm"</span>, <span class="dt">fw.method =</span> <span class="st">"chi.squared"</span>)
ps =<span class="st"> </span><span class="kw">makeParamSet</span>(<span class="kw">makeDiscreteParam</span>(<span class="st">"fw.perc"</span>, <span class="dt">values =</span> <span class="kw">seq</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.05</span>)))
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)
res =<span class="st"> </span><span class="kw"><a href="../../reference/tuneParams.html">tuneParams</a></span>(lrn, <span class="dt">task =</span> bh.task, <span class="dt">resampling =</span> rdesc, <span class="dt">par.set =</span> ps,
  <span class="dt">control =</span> <span class="kw"><a href="../../reference/makeTuneControlGrid.html">makeTuneControlGrid</a></span>())</code></pre></div>
<pre><code>## [Tune] Started tuning learner regr.lm.filtered for parameter set:</code></pre>
<pre><code>##             Type len Def                         Constr Req Tunable Trafo
## fw.perc discrete   -   - 0.2,0.25,0.3,0.35,0.4,0.45,0.5   -    TRUE     -</code></pre>
<pre><code>## With control class: TuneControlGrid</code></pre>
<pre><code>## Imputation value: Inf</code></pre>
<pre><code>## [Tune-x] 1: fw.perc=0.2</code></pre>
<pre><code>## [Tune-y] 1: mse.test.mean=46.2505494; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 2: fw.perc=0.25</code></pre>
<pre><code>## [Tune-y] 2: mse.test.mean=46.2505494; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 3: fw.perc=0.3</code></pre>
<pre><code>## [Tune-y] 3: mse.test.mean=29.6536007; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 4: fw.perc=0.35</code></pre>
<pre><code>## [Tune-y] 4: mse.test.mean=28.5007994; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 5: fw.perc=0.4</code></pre>
<pre><code>## [Tune-y] 5: mse.test.mean=28.5007994; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 6: fw.perc=0.45</code></pre>
<pre><code>## [Tune-y] 6: mse.test.mean=27.6796480; time: 0.0 min</code></pre>
<pre><code>## [Tune-x] 7: fw.perc=0.5</code></pre>
<pre><code>## [Tune-y] 7: mse.test.mean=27.6796480; time: 0.0 min</code></pre>
<pre><code>## [Tune] Result: fw.perc=0.45 : mse.test.mean=27.6796480</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res</code></pre></div>
<pre><code>## Tune result:
## Op. pars: fw.perc=0.45
## mse.test.mean=27.6796480</code></pre>
<p>The performance of all percentage values visited during tuning is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.data.frame</span>(res$opt.path)</code></pre></div>
<pre><code>##   fw.perc mse.test.mean dob eol error.message exec.time
## 1     0.2      46.25055   1  NA          &lt;NA&gt;     0.408
## 2    0.25      46.25055   2  NA          &lt;NA&gt;     0.212
## 3     0.3      29.65360   3  NA          &lt;NA&gt;     0.191
## 4    0.35      28.50080   4  NA          &lt;NA&gt;     0.201
## 5     0.4      28.50080   5  NA          &lt;NA&gt;     0.274
## 6    0.45      27.67965   6  NA          &lt;NA&gt;     0.181
## 7     0.5      27.67965   7  NA          &lt;NA&gt;     0.181</code></pre>
<p>The optimal percentage and the corresponding performance can be accessed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res$x</code></pre></div>
<pre><code>## $fw.perc
## [1] 0.45</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res$y</code></pre></div>
<pre><code>## mse.test.mean 
##      27.67965</code></pre>
<p>After tuning we can generate a new wrapped learner with the optimal percentage value for further use.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeFilterWrapper.html">makeFilterWrapper</a></span>(<span class="dt">learner =</span> <span class="st">"regr.lm"</span>, <span class="dt">fw.method =</span> <span class="st">"chi.squared"</span>, <span class="dt">fw.perc =</span> res$x$fw.perc)
mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, bh.task)
mod</code></pre></div>
<pre><code>## Model for learner.id=regr.lm.filtered; learner.class=FilterWrapper
## Trained on: task.id = BostonHousing-example; obs = 506; features = 13
## Hyperparameters: fw.method=chi.squared,fw.perc=0.45</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/getFilteredFeatures.html">getFilteredFeatures</a></span>(mod)</code></pre></div>
<pre><code>## [1] "crim"  "zn"    "rm"    "dis"   "rad"   "lstat"</code></pre>
<p>Here is another example using <a href="tune.md">multi-criteria tuning</a>. We consider <a href="&amp;MASS::lda">linear discriminant analysis</a> with precedent feature selection based on the Chi-squared statistic of independence (<code>"chi.squared"</code>) on the <a href="&amp;mlbench::sonar">Sonar</a> data set and tune the threshold value. During tuning both, the false positive and the false negative rate (<a href="measures.md">fpr</a> and <a href="measures.md">fnr</a>), are minimized. As search strategy we choose a random search (see <a href="&amp;TuneMultiCritControl">makeTuneMultiCritControlRandom</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeFilterWrapper.html">makeFilterWrapper</a></span>(<span class="dt">learner =</span> <span class="st">"classif.lda"</span>, <span class="dt">fw.method =</span> <span class="st">"chi.squared"</span>)
ps =<span class="st"> </span><span class="kw">makeParamSet</span>(<span class="kw">makeNumericParam</span>(<span class="st">"fw.threshold"</span>, <span class="dt">lower =</span> <span class="fl">0.1</span>, <span class="dt">upper =</span> <span class="fl">0.9</span>))
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">10</span>)
res =<span class="st"> </span><span class="kw"><a href="../../reference/tuneParamsMultiCrit.html">tuneParamsMultiCrit</a></span>(lrn, <span class="dt">task =</span> sonar.task, <span class="dt">resampling =</span> rdesc, <span class="dt">par.set =</span> ps,
  <span class="dt">measures =</span> <span class="kw">list</span>(fpr, fnr), <span class="dt">control =</span> <span class="kw"><a href="../../reference/TuneMultiCritControl.html">makeTuneMultiCritControlRandom</a></span>(<span class="dt">maxit =</span> 50L),
  <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
res</code></pre></div>
<pre><code>## Tune multicrit result:
## Points on front: 13</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">as.data.frame</span>(res$opt.path))</code></pre></div>
<pre><code>##   fw.threshold fpr.test.mean fnr.test.mean dob eol error.message exec.time
## 1    0.8382699     0.4644444     0.4771861   1  NA          &lt;NA&gt;     2.812
## 2    0.3741971     0.3342857     0.2691613   2  NA          &lt;NA&gt;     2.064
## 3    0.3811504     0.3120635     0.2608279   3  NA          &lt;NA&gt;     1.552
## 4    0.1379510     0.2600000     0.2624784   4  NA          &lt;NA&gt;     1.616
## 5    0.8413944     0.5546032     0.4763095   5  NA          &lt;NA&gt;     1.503
## 6    0.3823339     0.3063492     0.2545779   6  NA          &lt;NA&gt;     1.636</code></pre>
<p>The results can be visualized with function [&amp;plotTuneMultiCritResult]. The plot shows the false positive and false negative rates for all parameter values visited during tuning. The size of the points on the Pareto front is slightly increased.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotTuneMultiCritResult.html">plotTuneMultiCritResult</a></span>(res)</code></pre></div>
<p><img src="feature_selection_files/figure-html/unnamed-chunk-14-1.png" width="672"></p>
</div>
</div>
<div id="wrapper-methods" class="section level2">
<h2 class="hasAnchor">
<a href="#wrapper-methods" class="anchor"></a>Wrapper methods</h2>
<p>Wrapper methods use the performance of a learning algorithm to assess the usefulness of a feature set. In order to select a feature subset a learner is trained repeatedly on different feature subsets and the subset which leads to the best learner performance is chosen.</p>
<p>In order to use the wrapper approach we have to decide:</p>
<ul>
<li>How to assess the performance: This involves choosing a performance measure that serves as feature selection criterion and a resampling strategy.</li>
<li>Which learning method to use.</li>
<li>How to search the space of possible feature subsets.</li>
</ul>
<p>The search strategy is defined by functions following the naming convention <code>makeFeatSelControl&lt;search_strategy</code>. The following search strategies are available:</p>
<ul>
<li>Exhaustive search (<a href="&amp;FeatSelControl">makeFeatSelControlExhaustive</a>),</li>
<li>Genetic algorithm (<a href="&amp;FeatSelControl">makeFeatSelControlGA</a>),</li>
<li>Random search (<a href="&amp;FeatSelControl">makeFeatSelControlRandom</a>),</li>
<li>Deterministic forward or backward search (<a href="&amp;FeatSelControl">makeFeatSelControlSequential</a>).</li>
</ul>
<div id="select-a-feature-subset" class="section level3">
<h3 class="hasAnchor">
<a href="#select-a-feature-subset" class="anchor"></a>Select a feature subset</h3>
<p>Feature selection can be conducted with function [&amp;selectFeatures].</p>
<p>In the following example we perform an exhaustive search on the <a href="&amp;TH.data::wpbc">Wisconsin Prognostic Breast Cancer</a> data set. As learning method we use the <a href="&amp;survival::coxph">Cox proportional hazards model</a>. The performance is assessed by the holdout estimate of the concordance index (<a href="measures.md">cindex</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Specify the search strategy
ctrl =<span class="st"> </span><span class="kw"><a href="../../reference/FeatSelControl.html">makeFeatSelControlRandom</a></span>(<span class="dt">maxit =</span> 20L)
ctrl</code></pre></div>
<pre><code>## FeatSel control: FeatSelControlRandom
## Same resampling instance: TRUE
## Imputation value: &lt;worst&gt;
## Max. features: &lt;not used&gt;
## Max. iterations: 20
## Tune threshold: FALSE
## Further arguments: prob=0.5</code></pre>
<p><code>ctrl</code> is a [&amp;FeatSelControl] object that contains information about the search strategy and potential parameter values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Resample description
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Holdout"</span>)

## Select features
sfeats =<span class="st"> </span><span class="kw"><a href="../../reference/selectFeatures.html">selectFeatures</a></span>(<span class="dt">learner =</span> <span class="st">"surv.coxph"</span>, <span class="dt">task =</span> wpbc.task, <span class="dt">resampling =</span> rdesc,
  <span class="dt">control =</span> ctrl, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
sfeats</code></pre></div>
<pre><code>## FeatSel result:
## Features (20): mean_radius, mean_texture, mean_perimeter, mean_area, mean_compactness, mean_concavity, mean_fractaldim, SE_radius, SE_perimeter, SE_smoothness, SE_compactness, SE_fractaldim, worst_radius, worst_perimeter, worst_area, worst_smoothness, worst_concavepoints, worst_fractaldim, tsize, pnodes
## cindex.test.mean=0.6595092</code></pre>
<p><code>sfeats</code>is a <a href="&amp;selectFeatures">FeatSelResult</a> object. The selected features and the corresponding performance can be accessed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sfeats$x</code></pre></div>
<pre><code>##  [1] "mean_radius"         "mean_texture"        "mean_perimeter"     
##  [4] "mean_area"           "mean_compactness"    "mean_concavity"     
##  [7] "mean_fractaldim"     "SE_radius"           "SE_perimeter"       
## [10] "SE_smoothness"       "SE_compactness"      "SE_fractaldim"      
## [13] "worst_radius"        "worst_perimeter"     "worst_area"         
## [16] "worst_smoothness"    "worst_concavepoints" "worst_fractaldim"   
## [19] "tsize"               "pnodes"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sfeats$y</code></pre></div>
<pre><code>## cindex.test.mean 
##        0.6595092</code></pre>
<p>In a second example we fit a simple linear regression model to the <a href="&amp;mlbench::BostonHousing">BostonHousing</a> data set and use a sequential search to find a feature set that minimizes the mean squared error (<a href="measures.md">mse</a>). <code>method = "sfs"</code> indicates that we want to conduct a sequential forward search where features are added to the model until the performance cannot be improved anymore. See the documentation page <a href="&amp;FeatSelControl">makeFeatSelControlSequential</a> for other available sequential search methods. The search is stopped if the improvement is smaller than <code>alpha = 0.02</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Specify the search strategy
ctrl =<span class="st"> </span><span class="kw"><a href="../../reference/FeatSelControl.html">makeFeatSelControlSequential</a></span>(<span class="dt">method =</span> <span class="st">"sfs"</span>, <span class="dt">alpha =</span> <span class="fl">0.02</span>)

## Select features
rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">10</span>)
sfeats =<span class="st"> </span><span class="kw"><a href="../../reference/selectFeatures.html">selectFeatures</a></span>(<span class="dt">learner =</span> <span class="st">"regr.lm"</span>, <span class="dt">task =</span> bh.task, <span class="dt">resampling =</span> rdesc, <span class="dt">control =</span> ctrl,
  <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
sfeats</code></pre></div>
<pre><code>## FeatSel result:
## Features (8): zn, chas, nox, rm, dis, ptratio, b, lstat
## mse.test.mean=24.7182952</code></pre>
<p>Further information about the sequential feature selection process can be obtained by function [&amp;analyzeFeatSelResult].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/analyzeFeatSelResult.html">analyzeFeatSelResult</a></span>(sfeats)</code></pre></div>
<pre><code>## Features         : 8
## Performance      : mse.test.mean=24.7182952
## zn, chas, nox, rm, dis, ptratio, b, lstat
## 
## Path to optimum:
## - Features:    0  Init   :                       Perf = 84.852  Diff: NA  *
## - Features:    1  Add    : lstat                 Perf = 38.785  Diff: 46.066  *
## - Features:    2  Add    : rm                    Perf = 31.112  Diff: 7.6729  *
## - Features:    3  Add    : ptratio               Perf = 27.79  Diff: 3.3226  *
## - Features:    4  Add    : dis                   Perf = 26.929  Diff: 0.86021  *
## - Features:    5  Add    : nox                   Perf = 25.542  Diff: 1.3878  *
## - Features:    6  Add    : b                     Perf = 25.078  Diff: 0.46361  *
## - Features:    7  Add    : zn                    Perf = 24.888  Diff: 0.18961  *
## - Features:    8  Add    : chas                  Perf = 24.718  Diff: 0.17008  *
## 
## Stopped, because no improving feature was found.</code></pre>
</div>
<div id="fuse-a-learner-with-feature-selection" class="section level3">
<h3 class="hasAnchor">
<a href="#fuse-a-learner-with-feature-selection" class="anchor"></a>Fuse a learner with feature selection</h3>
<p>A <a href="&amp;makeLearner">Learner</a> can be fused with a feature selection strategy (i.e., a search strategy, a performance measure and a resampling strategy) by function [&amp;makeFeatSelWrapper]. During training features are selected according to the specified selection scheme. Then, the learner is trained on the selected feature subset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)
lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeFeatSelWrapper.html">makeFeatSelWrapper</a></span>(<span class="st">"surv.coxph"</span>, <span class="dt">resampling =</span> rdesc,
  <span class="dt">control =</span> <span class="kw"><a href="../../reference/FeatSelControl.html">makeFeatSelControlRandom</a></span>(<span class="dt">maxit =</span> <span class="dv">10</span>), <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, <span class="dt">task =</span> wpbc.task)
mod</code></pre></div>
<pre><code>## Model for learner.id=surv.coxph.featsel; learner.class=FeatSelWrapper
## Trained on: task.id = wpbc-example; obs = 194; features = 32
## Hyperparameters:</code></pre>
<p>The result of the feature selection can be extracted by function [&amp;getFeatSelResult].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sfeats =<span class="st"> </span><span class="kw"><a href="../../reference/getFeatSelResult.html">getFeatSelResult</a></span>(mod)
sfeats</code></pre></div>
<pre><code>## FeatSel result:
## Features (17): mean_texture, mean_perimeter, mean_smoothness, mean_concavity, mean_symmetry, SE_radius, SE_texture, SE_perimeter, SE_smoothness, SE_concavity, SE_concavepoints, SE_symmetry, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize
## cindex.test.mean=0.6143328</code></pre>
<p>The selected features are:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sfeats$x</code></pre></div>
<pre><code>##  [1] "mean_texture"        "mean_perimeter"      "mean_smoothness"    
##  [4] "mean_concavity"      "mean_symmetry"       "SE_radius"          
##  [7] "SE_texture"          "SE_perimeter"        "SE_smoothness"      
## [10] "SE_concavity"        "SE_concavepoints"    "SE_symmetry"        
## [13] "worst_concavity"     "worst_concavepoints" "worst_symmetry"     
## [16] "worst_fractaldim"    "tsize"</code></pre>
<p>The 5-fold cross-validated performance of the learner specified above can be computed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out.rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">5</span>)

r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="dt">learner =</span> lrn, <span class="dt">task =</span> wpbc.task, <span class="dt">resampling =</span> out.rdesc, <span class="dt">models =</span> <span class="ot">TRUE</span>,
  <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
r$aggr</code></pre></div>
<pre><code>## cindex.test.mean 
##        0.5641217</code></pre>
<p>The selected feature sets in the individual resampling iterations can be extracted as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lapply</span>(r$models, getFeatSelResult)</code></pre></div>
<pre><code>## [[1]]
## FeatSel result:
## Features (12): mean_radius, mean_perimeter, mean_compactness, mean_concavepoints, mean_fractaldim, SE_radius, SE_concavity, worst_texture, worst_area, worst_compactness, worst_concavepoints, tsize
## cindex.test.mean=0.6148370
## 
## [[2]]
## FeatSel result:
## Features (24): mean_radius, mean_texture, mean_perimeter, mean_area, mean_compactness, mean_concavepoints, mean_fractaldim, SE_radius, SE_texture, SE_perimeter, SE_area, SE_smoothness, SE_compactness, SE_concavity, SE_concavepoints, SE_fractaldim, worst_radius, worst_area, worst_concavity, worst_concavepoints, worst_symmetry, worst_fractaldim, tsize, pnodes
## cindex.test.mean=0.6928376
## 
## [[3]]
## FeatSel result:
## Features (15): mean_perimeter, mean_smoothness, mean_compactness, mean_concavity, mean_fractaldim, SE_perimeter, SE_smoothness, SE_fractaldim, worst_radius, worst_texture, worst_area, worst_smoothness, worst_symmetry, worst_fractaldim, pnodes
## cindex.test.mean=0.6585873
## 
## [[4]]
## FeatSel result:
## Features (15): mean_radius, mean_texture, mean_perimeter, mean_area, mean_concavity, SE_texture, SE_perimeter, SE_area, SE_compactness, worst_perimeter, worst_smoothness, worst_concavepoints, worst_symmetry, tsize, pnodes
## cindex.test.mean=0.6786323
## 
## [[5]]
## FeatSel result:
## Features (16): mean_radius, mean_texture, mean_smoothness, mean_concavity, mean_concavepoints, mean_fractaldim, SE_radius, SE_perimeter, SE_area, SE_compactness, worst_radius, worst_area, worst_compactness, worst_concavity, worst_concavepoints, tsize
## cindex.test.mean=0.6744434</code></pre>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#filter-methods">Filter methods</a></li>
      <li><a href="#wrapper-methods">Wrapper methods</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio, Mason Gallo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
