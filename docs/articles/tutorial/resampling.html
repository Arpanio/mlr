<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Resampling • mlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../index.html">mlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/mlr.html">Get Started</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorial
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Basics</li>
    <li>
      <a href="../../articles/tutorial/task.html">Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learners.html">Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/predict.html">Predict</a>
    </li>
    <li>
      <a href="../../articles/tutorial/performance.html">Performance</a>
    </li>
    <li>
      <a href="../../articles/tutorial/resample.html">Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/tune.html">Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/benchmark_experiments.html">Benchmark Experiments</a>
    </li>
    <li>
      <a href="../../articles/tutorial/parallelization.html">Parallelization</a>
    </li>
    <li>
      <a href="../../articles/tutorial/visualization.html">Visualization</a>
    </li>
    <li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/tutorial/advanced_tune.html">Advanced Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/bagging.html">Generic Bagging</a>
    </li>
    <li class="dropdown-header">Extend</li>
    <li class="dropdown-header">Appendix</li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlr-org/mlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Resampling</h1>
            
          </div>

    
    
<div class="contents">
<p>Resampling strategies are usually used to assess the performance of a learning algorithm: The entire data set is (repeatedly) split into training sets <span class="math inline">\(D^{*b}\)</span> and test sets <span class="math inline">\(D \setminus D^{*b}\)</span>, <span class="math inline">\(b = 1,\ldots,B\)</span>. The learner is trained on each training set, predictions are made on the corresponding test set (sometimes on the training set as well) and the performance measure <span class="math inline">\(S(D^{*b}, D \setminus D^{*b})\)</span> is calculated. Then the <span class="math inline">\(B\)</span> individual performance values are aggregated, most often by calculating the mean. There exist various different resampling strategies, for example cross-validation and bootstrap, to mention just two popular approaches.</p>
<figure><img src="img/resampling.png" title="Resampling Figure" alt="Resampling Figure"><figcaption>Resampling Figure</figcaption></figure><p>If you want to read up on further details, the paper <a href="http://link.springer.com/chapter/10.1007%2F978-0-387-47509-7_8">Resampling Strategies for Model Assessment and Selection</a> by Simon is probably not a bad choice. Bernd has also published a paper <a href="http://www.mitpressjournals.org/doi/pdf/10.1162/EVCO_a_00069">Resampling methods for meta-model validation with recommendations for evolutionary computation</a> which contains detailed descriptions and lots of statistical background information on resampling methods.</p>
<section id="defining-the-resampling-strategy" class="level2"><h2>Defining the resampling strategy</h2>
<p>In [%mlr] the resampling strategy can be defined via function [&amp;makeResampleDesc]. It requires a string that specifies the resampling method and, depending on the selected strategy, further information like the number of iterations. The supported resampling strategies are:</p>
<ul>
<li>Cross-validation (<code>"CV"</code>),</li>
<li>Leave-one-out cross-validation (<code>"LOO"</code>),</li>
<li>Repeated cross-validation (<code>"RepCV"</code>),</li>
<li>Out-of-bag bootstrap and other variants like <em>b632</em> (<code>"Bootstrap"</code>),</li>
<li>Subsampling, also called Monte-Carlo cross-validation (<code>"Subsample"</code>),</li>
<li>Holdout (training/test) (<code>"Holdout"</code>).</li>
</ul>
<p>For example if you want to use 3-fold cross-validation type:</p>
<pre class="sourceCode r" id="cb1"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">## 3-fold cross-validation</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">rdesc</a></code></pre>
<pre><code>## Resample description: cross-validation with 3 iterations.
## Predict: test
## Stratification: FALSE</code></pre>
<p>For holdout estimation use:</p>
<pre class="sourceCode r" id="cb3"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">## Holdout estimation</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Holdout"</span>)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">rdesc</a></code></pre>
<pre><code>## Resample description: holdout with 0.67 split rate.
## Predict: test
## Stratification: FALSE</code></pre>
<p>In order to save you some typing [%mlr] contains some pre-defined resample descriptions for very common strategies like holdout (<a href="&amp;makeResampleDesc">hout</a>) as well as cross-validation with different numbers of folds (e.g., <a href="&amp;makeResampleDesc">cv5</a> or <a href="&amp;makeResampleDesc">cv10</a>).</p>
<pre class="sourceCode r" id="cb5"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">hout</a></code></pre>
<pre><code>## Resample description: holdout with 0.67 split rate.
## Predict: test
## Stratification: FALSE</code></pre>
<pre class="sourceCode r" id="cb7"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">cv3</a></code></pre>
<pre><code>## Resample description: cross-validation with 3 iterations.
## Predict: test
## Stratification: FALSE</code></pre>
</section><section id="performing-the-resampling" class="level2"><h2>Performing the resampling</h2>
<p>Function [&amp;resample] evaluates a <a href="&amp;makeLearner">Learner</a> on a given machine learning [&amp;Task] using the selected <a href="&amp;makeResampleDesc">resampling strategy</a>.</p>
<p>As a first example, the performance of <a href="&amp;stats::lm">linear regression</a> on the <a href="&amp;mlbench::BostonHousing">BostonHousing</a> data set is calculated using <em>3-fold cross-validation</em>.</p>
<p>Generally, for <em><span class="math inline">\(K\)</span>-fold cross-validation</em> the data set <span class="math inline">\(D\)</span> is partitioned into <span class="math inline">\(K\)</span> subsets of (approximately) equal size. In the <span class="math inline">\(b\)</span>-th of the <span class="math inline">\(K\)</span> iterations, the <span class="math inline">\(b\)</span>-th subset is used for testing, while the union of the remaining parts forms the training set.</p>
<p>As usual, you can either pass a <a href="&amp;makeLearner">Learner</a> object to [&amp;resample] or, as done here, provide the class name <code>"regr.lm"</code> of the learner. Since no performance measure is specified the default for regression learners (mean squared error, <a href="measures.md">mse</a>) is calculated.</p>
<pre class="sourceCode r" id="cb9"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">## Specify the resampling strategy (3-fold cross-validation)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb9-3" data-line-number="3"></a>
<a class="sourceLine" id="cb9-4" data-line-number="4">## Calculate the performance</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"regr.lm"</span>, bh.task, rdesc)</a></code></pre>
<pre><code>## Resampling: cross-validation</code></pre>
<pre><code>## Measures:             mse</code></pre>
<pre><code>## [Resample] iter 1:    22.4396752</code></pre>
<pre><code>## [Resample] iter 2:    28.9833619</code></pre>
<pre><code>## [Resample] iter 3:    20.3787020</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mse.test.mean=23.9339130</code></pre>
<pre><code>## </code></pre>
<pre class="sourceCode r" id="cb18"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">r</a></code></pre>
<pre><code>## Resample Result
## Task: BostonHousing-example
## Learner: regr.lm
## Aggr perf: mse.test.mean=23.9339130
## Runtime: 0.0396838</code></pre>
<p>The result <code>r</code> is an object of class [&amp;ResampleResult]. It contains performance results for the learner and some additional information like the runtime, predicted values, and optionally the models fitted in single resampling iterations.</p>
<pre class="sourceCode r" id="cb20"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1">## Peak into r</a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="kw">names</span>(r)</a></code></pre>
<pre><code>##  [1] "learner.id"     "task.id"        "task.desc"      "measures.train"
##  [5] "measures.test"  "aggr"           "pred"           "models"        
##  [9] "err.msgs"       "err.dumps"      "extract"        "runtime"</code></pre>
<pre class="sourceCode r" id="cb22"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">r<span class="op">$</span>aggr</a></code></pre>
<pre><code>## mse.test.mean 
##      23.93391</code></pre>
<pre class="sourceCode r" id="cb24"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">r<span class="op">$</span>measures.test</a></code></pre>
<pre><code>##   iter      mse
## 1    1 22.43968
## 2    2 28.98336
## 3    3 20.37870</code></pre>
<p><code>r$measures.test</code> gives the performance on each of the 3 test data sets. <code>r$aggr</code> shows the aggregated performance value. Its name <code>"mse.test.mean"</code> indicates the performance measure, <a href="measures.md">mse</a>, and the method, <a href="&amp;aggregations">test.mean</a>, used to aggregate the 3 individual performances. <a href="&amp;aggregations">test.mean</a> is the default aggregation scheme for most performance measures and, as the name implies, takes the mean over the performances on the test data sets.</p>
<p>Resampling in [%mlr] works the same way for all types of learning problems and learners. Below is a classification example where a <a href="&amp;rpart::rpart">classification tree (rpart)</a> is evaluated on the <a href="&amp;mlbench::sonar">Sonar</a> data set by subsampling with 5 iterations.</p>
<p>In each subsampling iteration the data set <span class="math inline">\(D\)</span> is randomly partitioned into a training and a test set according to a given percentage, e.g., 2/3 training and 1/3 test set. If there is just one iteration, the strategy is commonly called <em>holdout</em> or <em>test sample estimation</em>.</p>
<p>You can calculate several measures at once by passing a <a href="&amp;base::list">list</a> of <a href="&amp;makeMeasure">Measure</a>s to [&amp;resample]. Below, the error rate (<a href="measures.md">mmce</a>), false positive and false negative rates (<a href="measures.md">fpr</a>, <a href="measures.md">fnr</a>), and the time it takes to train the learner (<a href="measures.md">timetrain</a>) are estimated by <em>subsampling</em> with 5 iterations.</p>
<pre class="sourceCode r" id="cb26"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1">## Subsampling with 5 iterations and default split ratio 2/3</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Subsample"</span>, <span class="dt">iters =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3"></a>
<a class="sourceLine" id="cb26-4" data-line-number="4">## Subsampling with 5 iterations and 4/5 training data</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Subsample"</span>, <span class="dt">iters =</span> <span class="dv">5</span>, <span class="dt">split =</span> <span class="dv">4</span><span class="op">/</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb26-6" data-line-number="6"></a>
<a class="sourceLine" id="cb26-7" data-line-number="7">## Classification tree with information splitting criterion</a>
<a class="sourceLine" id="cb26-8" data-line-number="8">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.rpart"</span>, <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">"information"</span>))</a>
<a class="sourceLine" id="cb26-9" data-line-number="9"></a>
<a class="sourceLine" id="cb26-10" data-line-number="10">## Calculate the performance measures</a>
<a class="sourceLine" id="cb26-11" data-line-number="11">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(lrn, sonar.task, rdesc, <span class="dt">measures =</span> <span class="kw">list</span>(mmce, fpr, fnr, timetrain))</a></code></pre>
<pre><code>## Resampling: subsampling</code></pre>
<pre><code>## Measures:             mmce        fpr         fnr         timetrain</code></pre>
<pre><code>## [Resample] iter 1:    0.2857143   0.4117647   0.2000000   0.0170000</code></pre>
<pre><code>## [Resample] iter 2:    0.1666667   0.3000000   0.0454545   0.0140000</code></pre>
<pre><code>## [Resample] iter 3:    0.3095238   0.2631579   0.3478261   0.0130000</code></pre>
<pre><code>## [Resample] iter 4:    0.1904762   0.1250000   0.2777778   0.0160000</code></pre>
<pre><code>## [Resample] iter 5:    0.3571429   0.5000000   0.2272727   0.0150000</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mmce.test.mean=0.2619048,fpr.test.mean=0.3199845,fnr.test.mean=0.2196662,timetrain.test.mean=0.0150000</code></pre>
<pre><code>## </code></pre>
<pre class="sourceCode r" id="cb37"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">r</a></code></pre>
<pre><code>## Resample Result
## Task: Sonar-example
## Learner: classif.rpart
## Aggr perf: mmce.test.mean=0.2619048,fpr.test.mean=0.3199845,fnr.test.mean=0.2196662,timetrain.test.mean=0.0150000
## Runtime: 0.330032</code></pre>
<p>If you want to add further measures afterwards, use [&amp;addRRMeasure].</p>
<pre class="sourceCode r" id="cb39"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">## Add balanced error rate (ber) and time used to predict</a>
<a class="sourceLine" id="cb39-2" data-line-number="2"><span class="kw"><a href="../../reference/addRRMeasure.html">addRRMeasure</a></span>(r, <span class="kw">list</span>(ber, timepredict))</a></code></pre>
<pre><code>## Resample Result
## Task: Sonar-example
## Learner: classif.rpart
## Aggr perf: mmce.test.mean=0.2619048,fpr.test.mean=0.3199845,fnr.test.mean=0.2196662,timetrain.test.mean=0.0150000,ber.test.mean=0.2698254,timepredict.test.mean=0.0454000
## Runtime: 0.330032</code></pre>
<p>By default, [&amp;resample] prints progress messages and intermediate results. You can turn this off by setting <code>show.info = FALSE</code>, as done in the code chunk below. (If you are interested in suppressing these messages permanently have a look at the tutorial page about <a href="configureMlr.md">configuring mlr</a>.)</p>
<p>In the above example, the <a href="&amp;makeLearner">Learner</a> was explicitly constructed. For convenience you can also specify the learner as a string and pass any learner parameters via the <code>...</code> argument of [&amp;resample].</p>
<pre class="sourceCode r" id="cb41"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.rpart"</span>, <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">"information"</span>), sonar.task, rdesc,</a>
<a class="sourceLine" id="cb41-2" data-line-number="2">  <span class="dt">measures =</span> <span class="kw">list</span>(mmce, fpr, fnr, timetrain), <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb41-3" data-line-number="3"></a>
<a class="sourceLine" id="cb41-4" data-line-number="4">r</a></code></pre>
<pre><code>## Resample Result
## Task: Sonar-example
## Learner: classif.rpart
## Aggr perf: mmce.test.mean=0.2666667,fpr.test.mean=0.3287953,fnr.test.mean=0.1958914,timetrain.test.mean=0.0150000
## Runtime: 0.11888</code></pre>
</section><section id="accessing-resample-results" class="level2"><h2>Accessing resample results</h2>
<p>Apart from the learner performance you can extract further information from the resample results, for example predicted values or the models fitted in individual resample iterations.</p>
<section id="predictions" class="level3"><h3>Predictions</h3>
<p>Per default, the [&amp;ResampleResult] contains the predictions made during the resampling. If you do not want to keep them, e.g., in order to conserve memory, set <code>keep.pred = FALSE</code> when calling [&amp;resample].</p>
<p>The predictions are stored in slot <code>$pred</code> of the resampling result, which can also be accessed by function [&amp;getRRPredictions].</p>
<pre class="sourceCode r" id="cb43"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">r<span class="op">$</span>pred</a></code></pre>
<pre><code>## Resampled Prediction for:
## Resample description: subsampling with 5 iterations and 0.80 split rate.
## Predict: test
## Stratification: FALSE
## predict.type: response
## threshold: 
## time (mean): 0.00
##    id truth response iter  set
## 1 202     M        R    1 test
## 2  30     R        M    1 test
## 3  67     R        R    1 test
## 4 125     M        R    1 test
## 5 157     M        M    1 test
## 6 193     M        M    1 test
## ... (#rows: 210, #cols: 5)</code></pre>
<pre class="sourceCode r" id="cb45"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1">pred =<span class="st"> </span><span class="kw"><a href="../../reference/getRRPredictions.html">getRRPredictions</a></span>(r)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">pred</a></code></pre>
<pre><code>## Resampled Prediction for:
## Resample description: subsampling with 5 iterations and 0.80 split rate.
## Predict: test
## Stratification: FALSE
## predict.type: response
## threshold: 
## time (mean): 0.00
##    id truth response iter  set
## 1 202     M        R    1 test
## 2  30     R        M    1 test
## 3  67     R        R    1 test
## 4 125     M        R    1 test
## 5 157     M        M    1 test
## 6 193     M        M    1 test
## ... (#rows: 210, #cols: 5)</code></pre>
<p><code>pred</code> is an object of class [&amp;ResamplePrediction]. Just as a [&amp;Prediction] object (see the tutorial page on <a href="predict.md">making predictions</a>) it has an element <code>$data</code> which is a <a href="&amp;base::data.frame">data.frame</a> that contains the predictions and in the case of a supervised learning problem the true values of the target variable(s). You can use <a href="&amp;Prediction">as.data.frame</a> to directly access the <code>$data</code> slot. Moreover, all getter functions for [&amp;Prediction] objects like <a href="&amp;getPredictionResponse">getPredictionResponse</a> or [&amp;getPredictionProbabilities] are applicable.</p>
<pre class="sourceCode r" id="cb47"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">as.data.frame</span>(pred))</a></code></pre>
<pre><code>##    id truth response iter  set
## 1 202     M        R    1 test
## 2  30     R        M    1 test
## 3  67     R        R    1 test
## 4 125     M        R    1 test
## 5 157     M        M    1 test
## 6 193     M        M    1 test</code></pre>
<pre class="sourceCode r" id="cb49"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw">head</span>(<span class="kw"><a href="../../reference/getPredictionResponse.html">getPredictionTruth</a></span>(pred))</a></code></pre>
<pre><code>## [1] M R R M M M
## Levels: M R</code></pre>
<pre class="sourceCode r" id="cb51"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="kw">head</span>(<span class="kw"><a href="../../reference/getPredictionResponse.html">getPredictionResponse</a></span>(pred))</a></code></pre>
<pre><code>## [1] R M R R M M
## Levels: M R</code></pre>
<p>The columns <code>iter</code> and <code>set</code> in the <a href="&amp;base::data.frame">data.frame</a> indicate the resampling iteration and the data set (<code>train</code> or <code>test</code>) for which the prediction was made.</p>
<p>By default, predictions are made for the test sets only. If predictions for the training set are required, set <code>predict = "train"</code> (for predictions on the train set only) or <code>predict = "both"</code> (for predictions on both train and test sets) in [&amp;makeResampleDesc]. In any case, this is necessary for some bootstrap methods (<em>b632</em> and <em>b632+</em>) and some examples are shown <a href="resample#aggregating-performance-values">later on</a>.</p>
<p>Below, we use simple Holdout, i.e., split the data once into a training and test set, as resampling strategy and make predictions on both sets.</p>
<pre class="sourceCode r" id="cb53"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1">## Make predictions on both training and test sets</a>
<a class="sourceLine" id="cb53-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Holdout"</span>, <span class="dt">predict =</span> <span class="st">"both"</span>)</a>
<a class="sourceLine" id="cb53-3" data-line-number="3"></a>
<a class="sourceLine" id="cb53-4" data-line-number="4">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.lda"</span>, iris.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb53-5" data-line-number="5">r</a></code></pre>
<pre><code>## Resample Result
## Task: iris-example
## Learner: classif.lda
## Aggr perf: mmce.test.mean=0.0200000
## Runtime: 0.0132458</code></pre>
<pre class="sourceCode r" id="cb55"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1">r<span class="op">$</span>measures.train</a></code></pre>
<pre><code>##   iter mmce
## 1    1 0.02</code></pre>
<p>(Please note that nonetheless the misclassification rate <code>r$aggr</code> is estimated on the test data only. How to calculate performance measures on the training sets is shown <a href="resample#aggregating-performance-values">below</a>.)</p>
<p>A second function to extract predictions from resample results is [&amp;getRRPredictionList] which returns a <a href="&amp;base::list">list</a> of predictions split by data set (train/test) and resampling iteration.</p>
<pre class="sourceCode r" id="cb57"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">predList =<span class="st"> </span><span class="kw"><a href="../../reference/getRRPredictionList.html">getRRPredictionList</a></span>(r)</a>
<a class="sourceLine" id="cb57-2" data-line-number="2">predList</a></code></pre>
<pre><code>## $train
## $train$`1`
## Prediction: 100 observations
## predict.type: response
## threshold: 
## time: 0.00
##      id      truth   response
## 79   79 versicolor versicolor
## 29   29     setosa     setosa
## 133 133  virginica  virginica
## 91   91 versicolor versicolor
## 25   25     setosa     setosa
## 15   15     setosa     setosa
## ... (#rows: 100, #cols: 3)
## 
## 
## $test
## $test$`1`
## Prediction: 50 observations
## predict.type: response
## threshold: 
## time: 0.00
##      id      truth   response
## 80   80 versicolor versicolor
## 141 141  virginica  virginica
## 95   95 versicolor versicolor
## 101 101  virginica  virginica
## 123 123  virginica  virginica
## 84   84 versicolor  virginica
## ... (#rows: 50, #cols: 3)</code></pre>
</section><section id="learner-models" class="level3"><h3>Learner models</h3>
<p>In each resampling iteration a <a href="&amp;makeLearner">Learner</a> is fitted on the respective training set. By default, the resulting <a href="&amp;makeWrappedModel">WrappedModel</a>s are not included in the [&amp;ResampleResult] and slot <code>$models</code> is empty. In order to keep them, set <code>models = TRUE</code> when calling [&amp;resample], as in the following survival analysis example.</p>
<pre class="sourceCode r" id="cb59"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">## 3-fold cross-validation</a>
<a class="sourceLine" id="cb59-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb59-3" data-line-number="3"></a>
<a class="sourceLine" id="cb59-4" data-line-number="4">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"surv.coxph"</span>, lung.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>, <span class="dt">models =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb59-5" data-line-number="5">r<span class="op">$</span>models</a></code></pre>
<pre><code>## [[1]]
## Model for learner.id=surv.coxph; learner.class=surv.coxph
## Trained on: task.id = lung-example; obs = 111; features = 8
## Hyperparameters: 
## 
## [[2]]
## Model for learner.id=surv.coxph; learner.class=surv.coxph
## Trained on: task.id = lung-example; obs = 111; features = 8
## Hyperparameters: 
## 
## [[3]]
## Model for learner.id=surv.coxph; learner.class=surv.coxph
## Trained on: task.id = lung-example; obs = 112; features = 8
## Hyperparameters:</code></pre>
</section><section id="the-extract-option" class="level3"><h3>The extract option</h3>
<p>Keeping complete fitted models can be memory-intensive if these objects are large or the number of resampling iterations is high. Alternatively, you can use the <code>extract</code> argument of [&amp;resample] to retain only the information you need. To this end you need to pass a <a href="&amp;base::function">function</a> to <code>extract</code> which is applied to each <a href="&amp;makeWrappedModel">WrappedModel</a> object fitted in each resampling iteration.</p>
<p>Below, we cluster the <a href="datasets::mtcars">mtcars</a> data using the <span class="math inline">\(k\)</span>-means algorithm with <span class="math inline">\(k = 3\)</span> and keep only the cluster centers.</p>
<pre class="sourceCode r" id="cb61"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1">## 3-fold cross-validation</a>
<a class="sourceLine" id="cb61-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb61-3" data-line-number="3"></a>
<a class="sourceLine" id="cb61-4" data-line-number="4">## Extract the compute cluster centers</a>
<a class="sourceLine" id="cb61-5" data-line-number="5">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"cluster.kmeans"</span>, mtcars.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb61-6" data-line-number="6">  <span class="dt">centers =</span> <span class="dv">3</span>, <span class="dt">extract =</span> <span class="cf">function</span>(x) <span class="kw"><a href="../../reference/getLearnerModel.html">getLearnerModel</a></span>(x)<span class="op">$</span>centers)</a></code></pre>
<pre><code>## 
## This is package 'modeest' written by P. PONCET.
## For a complete list of functions, use 'library(help = "modeest")' or 'help.start()'.</code></pre>
<pre class="sourceCode r" id="cb63"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">r<span class="op">$</span>extract</a></code></pre>
<pre><code>## [[1]]
##        mpg cyl     disp       hp     drat       wt   qsec  vs        am
## 1 17.32000 7.2 270.5200 145.0000 2.948000 3.609000 18.386 0.4 0.0000000
## 2 24.62000 4.6 117.5400 100.5000 4.017000 2.437800 18.715 0.8 0.7000000
## 3 14.86667 8.0 413.8333 210.6667 3.268333 4.412333 16.965 0.0 0.1666667
##       gear     carb
## 1 3.000000 2.000000
## 2 4.100000 2.500000
## 3 3.333333 3.333333
## 
## [[2]]
##        mpg      cyl     disp        hp     drat       wt     qsec
## 1 15.00000 8.000000 301.0000 335.00000 3.540000 3.570000 14.60000
## 2 25.44545 4.545455 117.5364  90.72727 4.058182 2.474091 18.66000
## 3 14.45556 8.000000 374.7556 212.11111 3.298889 4.133778 16.76556
##          vs        am     gear     carb
## 1 0.0000000 1.0000000 5.000000 8.000000
## 2 0.7272727 0.7272727 4.181818 2.363636
## 3 0.0000000 0.1111111 3.222222 3.444444
## 
## [[3]]
##        mpg cyl     disp        hp     drat       wt     qsec        vs
## 1 25.55714   4 112.8429  90.14286 3.947143 2.372571 18.61286 0.8571429
## 2 15.71111   8 317.8222 204.44444 3.186667 3.706667 16.67444 0.0000000
## 3 19.75000   6 189.7000 113.50000 3.580000 3.175000 18.39000 0.6666667
##          am     gear     carb
## 1 0.7142857 4.142857 1.571429
## 2 0.1111111 3.222222 3.444444
## 3 0.3333333 3.666667 3.000000</code></pre>
<p>As a second example, we extract the variable importances from fitted regression trees using function [&amp;getFeatureImportance]. (For more detailed information on this topic see the <a href="feature_selection.md">feature selection</a> page.)</p>
<pre class="sourceCode r" id="cb65"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1">## Extract the variable importance in a regression tree</a>
<a class="sourceLine" id="cb65-2" data-line-number="2">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"regr.rpart"</span>, bh.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>, <span class="dt">extract =</span> getFeatureImportance)</a>
<a class="sourceLine" id="cb65-3" data-line-number="3">r<span class="op">$</span>extract</a></code></pre>
<pre><code>## [[1]]
## FeatureImportance:
## Task: BostonHousing-example
## 
## Learner: regr.rpart
## Measure: NA
## Contrast: NA
## Aggregation: function (x)  x
## Replace: NA
## Number of Monte-Carlo iterations: NA
## Local: FALSE
##       crim       zn    indus chas      nox       rm      age      dis rad
## 1 668.3543 256.9252 4266.355    0 3138.553 14078.37 2828.724 3086.403   0
##        tax  ptratio        b    lstat
## 1 3105.721 3325.485 387.2739 10643.32
## 
## [[2]]
## FeatureImportance:
## Task: BostonHousing-example
## 
## Learner: regr.rpart
## Measure: NA
## Contrast: NA
## Aggregation: function (x)  x
## Replace: NA
## Number of Monte-Carlo iterations: NA
## Local: FALSE
##       crim       zn   indus chas      nox       rm      age      dis
## 1 8538.639 368.5668 9058.09    0 10016.29 14237.06 7572.955 2989.929
##        rad      tax  ptratio        b    lstat
## 1 1131.331 1594.552 1244.179 162.9199 18274.85
## 
## [[3]]
## FeatureImportance:
## Task: BostonHousing-example
## 
## Learner: regr.rpart
## Measure: NA
## Contrast: NA
## Aggregation: function (x)  x
## Replace: NA
## Number of Monte-Carlo iterations: NA
## Local: FALSE
##       crim       zn    indus     chas      nox       rm      age      dis
## 1 3052.297 1320.146 4906.361 108.8539 3966.526 17883.95 3059.395 3097.711
##        rad      tax  ptratio        b    lstat
## 1 641.0416 763.1453 3402.832 194.6956 11620.17</code></pre>
</section></section><section id="stratification-and-blocking" class="level2"><h2>Stratification and blocking</h2>
<ul>
<li>
<em>Stratification</em> with respect to a categorical variable makes sure that all its values are present in each training and test set in approximately the same proportion as in the original data set. Stratification is possible with regard to categorical target variables (and thus for supervised classification and survival analysis) or categorical explanatory variables.</li>
<li>
<em>Blocking</em> refers to the situation that subsets of observations belong together and must not be separated during resampling. Hence, for one train/test set pair the entire block is either in the training set or in the test set.</li>
</ul>
<section id="stratification-with-respect-to-the-target-variables" class="level3"><h3>Stratification with respect to the target variable(s)</h3>
<p>For classification, it is usually desirable to have the same proportion of the classes in all of the partitions of the original data set. This is particularly useful in the case of imbalanced classes and small data sets. Otherwise, it may happen that observations of less frequent classes are missing in some of the training sets which can decrease the performance of the learner, or lead to model crashes. In order to conduct stratified resampling, set <code>stratify = TRUE</code> in [&amp;makeResampleDesc].</p>
<pre class="sourceCode r" id="cb67"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1">## 3-fold cross-validation</a>
<a class="sourceLine" id="cb67-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>, <span class="dt">stratify =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb67-3" data-line-number="3"></a>
<a class="sourceLine" id="cb67-4" data-line-number="4">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.lda"</span>, iris.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb67-5" data-line-number="5">r</a></code></pre>
<pre><code>## Resample Result
## Task: iris-example
## Learner: classif.lda
## Aggr perf: mmce.test.mean=0.0197386
## Runtime: 0.0268075</code></pre>
<p>Stratification is also available for survival tasks. Here the stratification balances the censoring rate.</p>
</section><section id="stratification-with-respect-to-explanatory-variables" class="level3"><h3>Stratification with respect to explanatory variables</h3>
<p>Sometimes it is required to also stratify on the input data, e.g., to ensure that all subgroups are represented in all training and test sets. To stratify on the input columns, specify <a href="&amp;base::factor">factor</a> columns of your task data via <code>stratify.cols</code>.</p>
<pre class="sourceCode r" id="cb69"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>, <span class="dt">stratify.cols =</span> <span class="st">"chas"</span>)</a>
<a class="sourceLine" id="cb69-2" data-line-number="2"></a>
<a class="sourceLine" id="cb69-3" data-line-number="3">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"regr.rpart"</span>, bh.task, rdesc, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb69-4" data-line-number="4">r</a></code></pre>
<pre><code>## Resample Result
## Task: BostonHousing-example
## Learner: regr.rpart
## Aggr perf: mse.test.mean=22.8596357
## Runtime: 0.0348089</code></pre>
</section><section id="blocking" class="level3"><h3>Blocking</h3>
<p>If some observations “belong together” and must not be separated when splitting the data into training and test sets for resampling, you can supply this information via a <code>blocking</code> <a href="base::factor">factor</a> when <a href="task.md#further-settings">creating the task</a>.</p>
<pre class="sourceCode r" id="cb71"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1">## 5 blocks containing 30 observations each</a>
<a class="sourceLine" id="cb71-2" data-line-number="2">task =<span class="st"> </span><span class="kw"><a href="../../reference/Task.html">makeClassifTask</a></span>(<span class="dt">data =</span> iris, <span class="dt">target =</span> <span class="st">"Species"</span>, <span class="dt">blocking =</span> <span class="kw">factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">each =</span> <span class="dv">30</span>)))</a>
<a class="sourceLine" id="cb71-3" data-line-number="3">task</a></code></pre>
<pre><code>## Supervised task: iris
## Type: classif
## Target: Species
## Observations: 150
## Features:
##    numerics     factors     ordered functionals 
##           4           0           0           0 
## Missings: FALSE
## Has weights: FALSE
## Has blocking: TRUE
## Is spatial: FALSE
## Classes: 3
##     setosa versicolor  virginica 
##         50         50         50 
## Positive class: NA</code></pre>
</section></section><section id="resample-descriptions-and-resample-instances" class="level2"><h2>Resample descriptions and resample instances</h2>
<p>As already mentioned, you can specify a resampling strategy using function [&amp;makeResampleDesc].</p>
<pre class="sourceCode r" id="cb73"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb73-2" data-line-number="2">rdesc</a></code></pre>
<pre><code>## Resample description: cross-validation with 3 iterations.
## Predict: test
## Stratification: FALSE</code></pre>
<pre class="sourceCode r" id="cb75"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="kw">str</span>(rdesc)</a></code></pre>
<pre><code>## List of 4
##  $ id      : chr "cross-validation"
##  $ iters   : int 3
##  $ predict : chr "test"
##  $ stratify: logi FALSE
##  - attr(*, "class")= chr [1:2] "CVDesc" "ResampleDesc"</code></pre>
<pre class="sourceCode r" id="cb77"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="kw">str</span>(<span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Subsample"</span>, <span class="dt">stratify.cols =</span> <span class="st">"chas"</span>))</a></code></pre>
<pre><code>## List of 6
##  $ split        : num 0.667
##  $ id           : chr "subsampling"
##  $ iters        : int 30
##  $ predict      : chr "test"
##  $ stratify     : logi FALSE
##  $ stratify.cols: chr "chas"
##  - attr(*, "class")= chr [1:2] "SubsampleDesc" "ResampleDesc"</code></pre>
<p>The result <code>rdesc</code> inherits from class <a href="&amp;makeResampleDesc">ResampleDesc</a> (short for resample description) and, in principle, contains all necessary information about the resampling strategy including the number of iterations, the proportion of training and test sets, stratification variables, etc.</p>
<p>Given either the size of the data set at hand or the [&amp;Task], function [&amp;makeResampleInstance] draws the training and test sets according to the <a href="&amp;makeResampleDesc">ResampleDesc</a>.</p>
<pre class="sourceCode r" id="cb79"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1">## Create a resample instance based an a task</a>
<a class="sourceLine" id="cb79-2" data-line-number="2">rin =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleInstance.html">makeResampleInstance</a></span>(rdesc, iris.task)</a>
<a class="sourceLine" id="cb79-3" data-line-number="3">rin</a></code></pre>
<pre><code>## Resample instance for 150 cases.
## Resample description: cross-validation with 3 iterations.
## Predict: test
## Stratification: FALSE</code></pre>
<pre class="sourceCode r" id="cb81"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="kw">str</span>(rin)</a></code></pre>
<pre><code>## List of 5
##  $ desc      :List of 4
##   ..$ id      : chr "cross-validation"
##   ..$ iters   : int 3
##   ..$ predict : chr "test"
##   ..$ stratify: logi FALSE
##   ..- attr(*, "class")= chr [1:2] "CVDesc" "ResampleDesc"
##  $ size      : int 150
##  $ train.inds:List of 3
##   ..$ : int [1:100] 62 101 94 15 129 2 115 66 64 136 ...
##   ..$ : int [1:100] 62 149 101 94 15 129 115 77 136 106 ...
##   ..$ : int [1:100] 149 2 66 64 77 106 68 108 91 125 ...
##  $ test.inds :List of 3
##   ..$ : int [1:50] 5 8 24 25 31 35 39 40 44 53 ...
##   ..$ : int [1:50] 2 6 11 12 14 16 17 18 19 27 ...
##   ..$ : int [1:50] 1 3 4 7 9 10 13 15 20 21 ...
##  $ group     : Factor w/ 0 levels: 
##  - attr(*, "class")= chr "ResampleInstance"</code></pre>
<pre class="sourceCode r" id="cb83"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1">## Create a resample instance given the size of the data set</a>
<a class="sourceLine" id="cb83-2" data-line-number="2">rin =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleInstance.html">makeResampleInstance</a></span>(rdesc, <span class="dt">size =</span> <span class="kw">nrow</span>(iris))</a>
<a class="sourceLine" id="cb83-3" data-line-number="3"><span class="kw">str</span>(rin)</a></code></pre>
<pre><code>## List of 5
##  $ desc      :List of 4
##   ..$ id      : chr "cross-validation"
##   ..$ iters   : int 3
##   ..$ predict : chr "test"
##   ..$ stratify: logi FALSE
##   ..- attr(*, "class")= chr [1:2] "CVDesc" "ResampleDesc"
##  $ size      : int 150
##  $ train.inds:List of 3
##   ..$ : int [1:100] 74 93 1 140 71 12 33 37 100 56 ...
##   ..$ : int [1:100] 74 52 93 1 71 99 12 100 75 56 ...
##   ..$ : int [1:100] 52 140 99 33 37 75 4 13 107 132 ...
##  $ test.inds :List of 3
##   ..$ : int [1:50] 3 4 7 8 13 16 22 23 26 30 ...
##   ..$ : int [1:50] 2 6 9 11 14 15 19 21 27 28 ...
##   ..$ : int [1:50] 1 5 10 12 17 18 20 24 25 32 ...
##  $ group     : Factor w/ 0 levels: 
##  - attr(*, "class")= chr "ResampleInstance"</code></pre>
<pre class="sourceCode r" id="cb85"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1">## Access the indices of the training observations in iteration 3</a>
<a class="sourceLine" id="cb85-2" data-line-number="2">rin<span class="op">$</span>train.inds[[<span class="dv">3</span>]]</a></code></pre>
<pre><code>##   [1]  52 140  99  33  37  75   4  13 107 132  59  94  73 142 130  61  90
##  [18] 120  15  98 116 121 150 145 119 110  14   6  67  34  48 128 127  44
##  [35]  70  43  11  46 108 113  49  31  97 129  42  35  92 102  16  65  66
##  [52]  50   2  28  89  22  77  58  81  45 111  84 122 138 112  21 125  26
##  [69]  55 123  29 101  19  68   3   8  62 143 114  27 139   9  69  30  57
##  [86] 103  63  53  36  51  23  78  79  40 136  86  87  64   7  91</code></pre>
<p>The result <code>rin</code> inherits from class <a href="&amp;makeResampleInstance">ResampleInstance</a> and contains <a href="&amp;base::list">list</a>s of index vectors for the train and test sets.</p>
<p>If a <a href="&amp;makeResampleDesc">ResampleDesc</a> is passed to [&amp;resample], it is instantiated internally. Naturally, it is also possible to pass a <a href="&amp;makeResampleInstance">ResampleInstance</a> directly.</p>
<p>While the separation between resample descriptions, resample instances, and the [&amp;resample] function itself seems overly complicated, it has several advantages:</p>
<ul>
<li>Resample instances readily allow for paired experiments, that is comparing the performance of several learners on exactly the same training and test sets. This is particularly useful if you want to add another method to a comparison experiment you already did. Moreover, you can store the resample instance along with your data in order to be able to reproduce your results later on.</li>
</ul>
<pre class="sourceCode r" id="cb87"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb87-2" data-line-number="2">rin =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleInstance.html">makeResampleInstance</a></span>(rdesc, <span class="dt">task =</span> iris.task)</a>
<a class="sourceLine" id="cb87-3" data-line-number="3"></a>
<a class="sourceLine" id="cb87-4" data-line-number="4">## Calculate the performance of two learners based on the same resample instance</a>
<a class="sourceLine" id="cb87-5" data-line-number="5">r.lda =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.lda"</span>, iris.task, rin, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb87-6" data-line-number="6">r.rpart =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.rpart"</span>, iris.task, rin, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb87-7" data-line-number="7">r.lda<span class="op">$</span>aggr</a></code></pre>
<pre><code>## mmce.test.mean 
##           0.02</code></pre>
<pre class="sourceCode r" id="cb89"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1">r.rpart<span class="op">$</span>aggr</a></code></pre>
<pre><code>## mmce.test.mean 
##           0.06</code></pre>
<ul>
<li>In order to add further resampling methods you can simply derive from the <a href="&amp;makeResampleDesc">ResampleDesc</a> and <a href="&amp;makeResampleInstance">ResampleInstance</a> classes, but you do neither have to touch [&amp;resample] nor any further methods that use the resampling strategy.</li>
</ul>
<p>Usually, when calling [&amp;makeResampleInstance] the train and test index sets are drawn randomly. Mainly for <em>holdout</em> (<em>test sample</em>) <em>estimation</em> you might want full control about the training and tests set and specify them manually. This can be done using function [&amp;makeFixedHoldoutInstance].</p>
<pre class="sourceCode r" id="cb91"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">rin =<span class="st"> </span><span class="kw"><a href="../../reference/makeFixedHoldoutInstance.html">makeFixedHoldoutInstance</a></span>(<span class="dt">train.inds =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>, <span class="dt">test.inds =</span> <span class="dv">101</span><span class="op">:</span><span class="dv">150</span>, <span class="dt">size =</span> <span class="dv">150</span>)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2">rin</a></code></pre>
<pre><code>## Resample instance for 150 cases.
## Resample description: holdout with 0.67 split rate.
## Predict: test
## Stratification: FALSE</code></pre>
</section><section id="aggregating-performance-values" class="level2"><h2>Aggregating performance values</h2>
<p>In each resampling iteration <span class="math inline">\(b = 1,\ldots,B\)</span> we get performance values <span class="math inline">\(S(D^{*b}, D \setminus D^{*b})\)</span> (for each measure we wish to calculate), which are then aggregated to an overall performance.</p>
<p>For the great majority of common resampling strategies (like holdout, cross-validation, subsampling) performance values are calculated on the test data sets only and for most measures aggregated by taking the mean (<a href="&amp;aggregations">test.mean</a>).</p>
<p>Each performance <a href="&amp;makeMeasure">Measure</a> in [%mlr] has a corresponding default aggregation method which is stored in slot <code>$aggr</code>. The default aggregation for most measures is <a href="&amp;aggregations">test.mean</a>. One exception is the root mean square error (<a href="measures.md">rmse</a>).</p>
<pre class="sourceCode r" id="cb93"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1">## Mean misclassification error</a>
<a class="sourceLine" id="cb93-2" data-line-number="2">mmce<span class="op">$</span>aggr</a></code></pre>
<pre><code>## Aggregation function: test.mean</code></pre>
<pre class="sourceCode r" id="cb95"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1">mmce<span class="op">$</span>aggr<span class="op">$</span>fun</a></code></pre>
<pre><code>## function(task, perf.test, perf.train, measure, group, pred) mean(perf.test)
## &lt;bytecode: 0x564d95100538&gt;
## &lt;environment: namespace:mlr&gt;</code></pre>
<pre class="sourceCode r" id="cb97"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1">## Root mean square error</a>
<a class="sourceLine" id="cb97-2" data-line-number="2">rmse<span class="op">$</span>aggr</a></code></pre>
<pre><code>## Aggregation function: test.rmse</code></pre>
<pre class="sourceCode r" id="cb99"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1">rmse<span class="op">$</span>aggr<span class="op">$</span>fun</a></code></pre>
<pre><code>## function(task, perf.test, perf.train, measure, group, pred) sqrt(mean(perf.test^2))
## &lt;bytecode: 0x564d9879a208&gt;
## &lt;environment: namespace:mlr&gt;</code></pre>
<p>You can change the aggregation method of a <a href="&amp;makeMeasure">Measure</a> via function [&amp;setAggregation]. All available aggregation schemes are listed on the [&amp;aggregations] documentation page.</p>
<section id="example-one-measure-with-different-aggregations" class="level3"><h3>Example: One measure with different aggregations</h3>
<p>The aggregation schemes <a href="&amp;aggregations">test.median</a>, <a href="&amp;aggregations">test.min</a>, and <a href="&amp;aggregations">test.max</a> compute the median, minimum, and maximum of the performance values on the test sets.</p>
<pre class="sourceCode r" id="cb101"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1">mseTestMedian =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mse, test.median)</a>
<a class="sourceLine" id="cb101-2" data-line-number="2">mseTestMin =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mse, test.min)</a>
<a class="sourceLine" id="cb101-3" data-line-number="3">mseTestMax =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mse, test.max)</a>
<a class="sourceLine" id="cb101-4" data-line-number="4"></a>
<a class="sourceLine" id="cb101-5" data-line-number="5">mseTestMedian</a></code></pre>
<pre><code>## Name: Mean of squared errors
## Performance measure: mse
## Properties: regr,req.pred,req.truth
## Minimize: TRUE
## Best: 0; Worst: Inf
## Aggregated by: test.median
## Arguments: 
## Note: Defined as: mean((response - truth)^2)</code></pre>
<pre class="sourceCode r" id="cb103"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb103-2" data-line-number="2">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"regr.lm"</span>, bh.task, rdesc, <span class="dt">measures =</span> <span class="kw">list</span>(mse, mseTestMedian, mseTestMin, mseTestMax))</a></code></pre>
<pre><code>## Resampling: cross-validation</code></pre>
<pre><code>## Measures:             mse       mse       mse       mse</code></pre>
<pre><code>## [Resample] iter 1:    23.452294923.452294923.452294923.4522949</code></pre>
<pre><code>## [Resample] iter 2:    26.878383226.878383226.878383226.8783832</code></pre>
<pre><code>## [Resample] iter 3:    22.422750422.422750422.422750422.4227504</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mse.test.mean=24.2511428,mse.test.median=23.4522949,mse.test.min=22.4227504,mse.test.max=26.8783832</code></pre>
<pre><code>## </code></pre>
<pre class="sourceCode r" id="cb112"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1">r</a></code></pre>
<pre><code>## Resample Result
## Task: BostonHousing-example
## Learner: regr.lm
## Aggr perf: mse.test.mean=24.2511428,mse.test.median=23.4522949,mse.test.min=22.4227504,mse.test.max=26.8783832
## Runtime: 0.0277104</code></pre>
<pre class="sourceCode r" id="cb114"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1">r<span class="op">$</span>aggr</a></code></pre>
<pre><code>##   mse.test.mean mse.test.median    mse.test.min    mse.test.max 
##        24.25114        23.45229        22.42275        26.87838</code></pre>
</section><section id="example-calculating-the-training-error" class="level3"><h3>Example: Calculating the training error</h3>
<p>Below we calculate the mean misclassification error (<a href="measures.md">mmce</a>) on the training and the test data sets. Note that we have to set <code>predict = "both"</code> when calling [&amp;makeResampleDesc] in order to get predictions on both training and test sets.</p>
<pre class="sourceCode r" id="cb116"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1">mmceTrainMean =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mmce, train.mean)</a>
<a class="sourceLine" id="cb116-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">3</span>, <span class="dt">predict =</span> <span class="st">"both"</span>)</a>
<a class="sourceLine" id="cb116-3" data-line-number="3">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.rpart"</span>, iris.task, rdesc, <span class="dt">measures =</span> <span class="kw">list</span>(mmce, mmceTrainMean))</a></code></pre>
<pre><code>## Resampling: cross-validation</code></pre>
<pre><code>## Measures:             mmce.train   mmce.test</code></pre>
<pre><code>## [Resample] iter 1:    0.0400000    0.1200000</code></pre>
<pre><code>## [Resample] iter 2:    0.0300000    0.0600000</code></pre>
<pre><code>## [Resample] iter 3:    0.0400000    0.0400000</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mmce.test.mean=0.0733333,mmce.train.mean=0.0366667</code></pre>
<pre><code>## </code></pre>
<pre class="sourceCode r" id="cb125"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1">r<span class="op">$</span>measures.train</a></code></pre>
<pre><code>##   iter mmce mmce
## 1    1 0.04 0.04
## 2    2 0.03 0.03
## 3    3 0.04 0.04</code></pre>
<pre class="sourceCode r" id="cb127"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1">r<span class="op">$</span>aggr</a></code></pre>
<pre><code>##  mmce.test.mean mmce.train.mean 
##      0.07333333      0.03666667</code></pre>
</section><section id="example-bootstrap" class="level3"><h3>Example: Bootstrap</h3>
<p>In <em>out-of-bag bootstrap estimation</em> <span class="math inline">\(B\)</span> new data sets <span class="math inline">\(D^{*1}, \ldots, D^{*B}\)</span> are drawn from the data set <span class="math inline">\(D\)</span> with replacement, each of the same size as <span class="math inline">\(D\)</span>. In the <span class="math inline">\(b\)</span>-th iteration, <span class="math inline">\(D^{*b}\)</span> forms the training set, while the remaining elements from <span class="math inline">\(D\)</span>, i.e., <span class="math inline">\(D \setminus D^{*b}\)</span>, form the test set.</p>
<!--(
                     |resampling_desc_figure|

                     |resampling_nested_resampling_figure|
)-->
<p>The <em>b632</em> and <em>b632+</em> variants calculate a convex combination of the training performance and the out-of-bag bootstrap performance and thus require predictions on the training sets and an appropriate aggregation strategy.</p>
<pre class="sourceCode r" id="cb129"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">## Use bootstrap as resampling strategy and predict on both train and test sets</a>
<a class="sourceLine" id="cb129-2" data-line-number="2">rdesc =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Bootstrap"</span>, <span class="dt">predict =</span> <span class="st">"both"</span>, <span class="dt">iters =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb129-3" data-line-number="3"></a>
<a class="sourceLine" id="cb129-4" data-line-number="4">## Set aggregation schemes for b632 and b632+ bootstrap</a>
<a class="sourceLine" id="cb129-5" data-line-number="5">mmceB632 =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mmce, b632)</a>
<a class="sourceLine" id="cb129-6" data-line-number="6">mmceB632plus =<span class="st"> </span><span class="kw"><a href="../../reference/setAggregation.html">setAggregation</a></span>(mmce, b632plus)</a>
<a class="sourceLine" id="cb129-7" data-line-number="7"></a>
<a class="sourceLine" id="cb129-8" data-line-number="8">mmceB632</a></code></pre>
<pre><code>## Name: Mean misclassification error
## Performance measure: mmce
## Properties: classif,classif.multi,req.pred,req.truth
## Minimize: TRUE
## Best: 0; Worst: 1
## Aggregated by: b632
## Arguments: 
## Note: Defined as: mean(response != truth)</code></pre>
<pre class="sourceCode r" id="cb131"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1">r =<span class="st"> </span><span class="kw"><a href="../../reference/resample.html">resample</a></span>(<span class="st">"classif.rpart"</span>, iris.task, rdesc, <span class="dt">measures =</span> <span class="kw">list</span>(mmce, mmceB632, mmceB632plus),</a>
<a class="sourceLine" id="cb131-2" data-line-number="2">  <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb131-3" data-line-number="3"><span class="kw">head</span>(r<span class="op">$</span>measures.train)</a></code></pre>
<pre><code>##   iter       mmce       mmce       mmce
## 1    1 0.02666667 0.02666667 0.02666667
## 2    2 0.04000000 0.04000000 0.04000000
## 3    3 0.01333333 0.01333333 0.01333333
## 4    4 0.02666667 0.02666667 0.02666667
## 5    5 0.01333333 0.01333333 0.01333333
## 6    6 0.02000000 0.02000000 0.02000000</code></pre>
<pre class="sourceCode r" id="cb133"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">## Compare misclassification rates for out-of-bag, b632, and b632+ bootstrap</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">r<span class="op">$</span>aggr</a></code></pre>
<pre><code>## mmce.test.mean      mmce.b632  mmce.b632plus 
##     0.06959057     0.05256791     0.05400731</code></pre>
</section></section><section id="convenience-functions" class="level2"><h2>Convenience functions</h2>
<p>The functionality described on this page allows for much control and flexibility. However, when quickly trying out some learners, it can get tedious to type all the code for defining the resampling strategy, setting the aggregation scheme and so on. As mentioned above, [%mlr] includes some pre-defined resample description objects for frequently used strategies like, e.g., 5-fold cross-validation (<a href="&amp;makeResampleDesc">cv5</a>). Moreover, [%mlr] provides special functions for the most common resampling methods, for example <a href="&amp;resample">holdout</a>, <a href="&amp;resample">crossval</a>, or <a href="&amp;resample">bootstrapB632</a>.</p>
<pre class="sourceCode r" id="cb135"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1"><span class="kw"><a href="../../reference/resample.html">crossval</a></span>(<span class="st">"classif.lda"</span>, iris.task, <span class="dt">iters =</span> <span class="dv">3</span>, <span class="dt">measures =</span> <span class="kw">list</span>(mmce, ber))</a></code></pre>
<pre><code>## Resampling: cross-validation</code></pre>
<pre><code>## Measures:             mmce      ber</code></pre>
<pre><code>## [Resample] iter 1:    0.0400000 0.0380952</code></pre>
<pre><code>## [Resample] iter 2:    0.0200000 0.0256410</code></pre>
<pre><code>## [Resample] iter 3:    0.0000000 0.0000000</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mmce.test.mean=0.0200000,ber.test.mean=0.0212454</code></pre>
<pre><code>## </code></pre>
<pre><code>## Resample Result
## Task: iris-example
## Learner: classif.lda
## Aggr perf: mmce.test.mean=0.0200000,ber.test.mean=0.0212454
## Runtime: 0.0235002</code></pre>
<pre class="sourceCode r" id="cb145"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1"><span class="kw"><a href="../../reference/resample.html">bootstrapB632plus</a></span>(<span class="st">"regr.lm"</span>, bh.task, <span class="dt">iters =</span> <span class="dv">3</span>, <span class="dt">measures =</span> <span class="kw">list</span>(mse, mae))</a></code></pre>
<pre><code>## Resampling: OOB bootstrapping</code></pre>
<pre><code>## Measures:             mse.train   mae.train   mse.test    mae.test</code></pre>
<pre><code>## [Resample] iter 1:    18.8801176  3.1155617   24.7566756  3.4174014</code></pre>
<pre><code>## [Resample] iter 2:    27.2501229  3.6901770   21.6880537  3.4577840</code></pre>
<pre><code>## [Resample] iter 3:    24.3135925  3.4473571   23.3052536  3.5004230</code></pre>
<pre><code>## </code></pre>
<pre><code>## Aggregated Result: mse.b632plus=23.3752311,mae.b632plus=3.4453766</code></pre>
<pre><code>## </code></pre>
<pre><code>## Resample Result
## Task: BostonHousing-example
## Learner: regr.lm
## Aggr perf: mse.b632plus=23.3752311,mae.b632plus=3.4453766
## Runtime: 0.042527</code></pre>
</section>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#defining-the-resampling-strategy">Defining the resampling strategy</a></li>
      <li><a href="#performing-the-resampling">Performing the resampling</a></li>
      <li><a href="#accessing-resample-results">Accessing resample results</a></li>
      <li><a href="#stratification-and-blocking">Stratification and blocking</a></li>
      <li><a href="#resample-descriptions-and-resample-instances">Resample descriptions and resample instances</a></li>
      <li><a href="#aggregating-performance-values">Aggregating performance values</a></li>
      <li><a href="#convenience-functions">Convenience functions</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio, Mason Gallo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
