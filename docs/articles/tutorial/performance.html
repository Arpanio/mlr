<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Evaluating Learner Performance â€¢ mlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../index.html">mlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/mlr.html">Get Started</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Tutorial
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Basics</li>
    <li>
      <a href="../../articles/tutorial/task.html">Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learners.html">Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/predict.html">Predict</a>
    </li>
    <li>
      <a href="../../articles/tutorial/performance.html">Performance</a>
    </li>
    <li>
      <a href="../../articles/tutorial/resample.html">Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/tune.html">Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/benchmark_experiments.html">Benchmark Experiments</a>
    </li>
    <li>
      <a href="../../articles/tutorial/parallelization.html">Parallelization</a>
    </li>
    <li>
      <a href="../../articles/tutorial/visualization.html">Visualization</a>
    </li>
    <li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/tutorial/advanced_tune.html">Advanced Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/bagging.html">Generic Bagging</a>
    </li>
    <li class="dropdown-header">Extend</li>
    <li class="dropdown-header">Appendix</li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlr-org/mlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Evaluating Learner Performance</h1>
            
          </div>

    
    
<div class="contents">
<p>The quality of the predictions of a model in [%mlr] can be assessed with respect to a number of different performance measures. In order to calculate the performance measures, call [&amp;performance] on the object returned by <a href="&amp;predict.WrappedModel">predict</a> and specify the desired performance measures.</p>
<section id="available-performance-measures" class="level2"><h2>Available performance measures</h2>
<p>[%mlr] provides a large number of performance measures for all types of learning problems. Typical performance measures for <em>classification</em> are the mean misclassification error (<a href="measures.md">mmce</a>), accuracy (<a href="measures.md">acc</a>) or measures based on <a href="roc_analysis.md">ROC analysis</a>. For <em>regression</em> the mean of squared errors (<a href="measures.md">mse</a>) or mean of absolute errors (<a href="measures.md">mae</a>) are usually considered. For <em>clustering</em> tasks, measures such as the Dunn index (<a href="measures.md">dunn</a>) are provided, while for <em>survival</em> predictions, the Concordance Index (<a href="measures.md">cindex</a>) is supported, and for <em>cost-sensitive</em> predictions the misclassification penalty (<a href="measures.md">mcp</a>) and others. It is also possible to access the time to train the learner (<a href="measures.md">timetrain</a>), the time to compute the prediction (<a href="measures.md">timepredict</a>) and their sum (<a href="measures.md">timeboth</a>) as performance measures.</p>
<p>To see which performance measures are implemented, have a look at the <a href="measures.md">table of performance measures</a> and the [&amp;measures] documentation page.</p>
<p>If you want to implement an additional measure or include a measure with non-standard misclassification costs, see the section on <a href="create_measure.md">creating custom measures</a>.</p>
</section><section id="listing-measures" class="level2"><h2>Listing measures</h2>
<p>The properties and requirements of the individual measures are shown in the <a href="measures.md">table of performance measures</a>.</p>
<p>If you would like a list of available measures with certain properties or suitable for a certain learning [&amp;Task] use the function [&amp;listMeasures].</p>
<pre class="sourceCode r" id="cb1"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">## Performance measures for classification with multiple classes</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw"><a href="../../reference/listMeasures.html">listMeasures</a></span>(<span class="st">"classif"</span>, <span class="dt">properties =</span> <span class="st">"classif.multi"</span>)</a></code></pre>
<pre><code>##  [1] "kappa"            "multiclass.brier" "multiclass.aunp" 
##  [4] "multiclass.aunu"  "qsr"              "ber"             
##  [7] "logloss"          "wkappa"           "timeboth"        
## [10] "timepredict"      "acc"              "lsr"             
## [13] "featperc"         "multiclass.au1p"  "multiclass.au1u" 
## [16] "ssr"              "timetrain"        "mmce"</code></pre>
<pre class="sourceCode r" id="cb3"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">## Performance measure suitable for the iris classification task</a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw"><a href="../../reference/listMeasures.html">listMeasures</a></span>(iris.task)</a></code></pre>
<pre><code>##  [1] "kappa"            "multiclass.brier" "multiclass.aunp" 
##  [4] "multiclass.aunu"  "qsr"              "ber"             
##  [7] "logloss"          "wkappa"           "timeboth"        
## [10] "timepredict"      "acc"              "lsr"             
## [13] "featperc"         "multiclass.au1p"  "multiclass.au1u" 
## [16] "ssr"              "timetrain"        "mmce"</code></pre>
<p>For convenience there exists a default measure for each type of learning problem, which is calculated if nothing else is specified. As defaults we chose the most commonly used measures for the respective types, e.g., the mean squared error (<a href="measures.md">mse</a>) for regression and the misclassification rate (<a href="measures.md">mmce</a>) for classification. The help page of function [&amp;getDefaultMeasure] lists all defaults for all types of learning problems. The function itself returns the default measure for a given task type, [&amp;Task] or [&amp;Learner].</p>
<pre class="sourceCode r" id="cb5"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">## Get default measure for iris.task</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw"><a href="../../reference/getDefaultMeasure.html">getDefaultMeasure</a></span>(iris.task)</a></code></pre>
<pre><code>## Name: Mean misclassification error
## Performance measure: mmce
## Properties: classif,classif.multi,req.pred,req.truth
## Minimize: TRUE
## Best: 0; Worst: 1
## Aggregated by: test.mean
## Arguments: 
## Note: Defined as: mean(response != truth)</code></pre>
<pre class="sourceCode r" id="cb7"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">## Get the default measure for linear regression</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw"><a href="../../reference/getDefaultMeasure.html">getDefaultMeasure</a></span>(<span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"regr.lm"</span>))</a></code></pre>
<pre><code>## Name: Mean of squared errors
## Performance measure: mse
## Properties: regr,req.pred,req.truth
## Minimize: TRUE
## Best: 0; Worst: Inf
## Aggregated by: test.mean
## Arguments: 
## Note: Defined as: mean((response - truth)^2)</code></pre>
</section><section id="calculate-performance-measures" class="level2"><h2>Calculate performance measures</h2>
<p>In the following example we fit a <a href="&amp;gbm::gbm">gradient boosting machine</a> on a subset of the <a href="&amp;mlbench::BostonHousing">BostonHousing</a> data set and calculate the default measure mean squared error (<a href="measures.md">mse</a>) on the remaining observations.</p>
<pre class="sourceCode r" id="cb9"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">n =<span class="st"> </span><span class="kw"><a href="../../reference/getTaskSize.html">getTaskSize</a></span>(bh.task)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"regr.gbm"</span>, <span class="dt">n.trees =</span> <span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb9-3" data-line-number="3">mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, <span class="dt">task =</span> bh.task, <span class="dt">subset =</span> <span class="kw">seq</span>(<span class="dv">1</span>, n, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">task =</span> bh.task, <span class="dt">subset =</span> <span class="kw">seq</span>(<span class="dv">2</span>, n, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb9-5" data-line-number="5"></a>
<a class="sourceLine" id="cb9-6" data-line-number="6"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred)</a></code></pre>
<pre><code>##      mse 
## 42.81077</code></pre>
<p>The following code computes the median of squared errors (<a href="measures.md">medse</a>) instead.</p>
<pre class="sourceCode r" id="cb11"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> medse)</a></code></pre>
<pre><code>##    medse 
## 9.011062</code></pre>
<p>Of course, we can also calculate multiple performance measures at once by simply passing a list of measures which can also include <a href="create_measure.md">your own measure</a>.</p>
<p>Calculate the mean squared error, median squared error and mean absolute error (<a href="measures.md">mae</a>).</p>
<pre class="sourceCode r" id="cb13"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> <span class="kw">list</span>(mse, medse, mae))</a></code></pre>
<pre><code>##       mse     medse       mae 
## 42.810766  9.011062  4.552955</code></pre>
<p>For the other types of learning problems and measures, calculating the performance basically works in the same way.</p>
<section id="requirements-of-performance-measures" class="level3"><h3>Requirements of performance measures</h3>
<p>Note that in order to calculate some performance measures it is required that you pass the [&amp;Task] or the <a href="&amp;makeWrappedModel">fitted model</a> in addition to the [&amp;Prediction].</p>
<p>For example in order to assess the time needed for training (<a href="measures.md">timetrain</a>), the fitted model has to be passed.</p>
<pre class="sourceCode r" id="cb15"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> timetrain, <span class="dt">model =</span> mod)</a></code></pre>
<pre><code>## timetrain 
##     0.066</code></pre>
<p>For many performance measures in cluster analysis the [&amp;Task] is required.</p>
<pre class="sourceCode r" id="cb17"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"cluster.kmeans"</span>, <span class="dt">centers =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, mtcars.task)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">task =</span> mtcars.task)</a>
<a class="sourceLine" id="cb17-4" data-line-number="4"></a>
<a class="sourceLine" id="cb17-5" data-line-number="5">## Calculate the Dunn index</a>
<a class="sourceLine" id="cb17-6" data-line-number="6"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> dunn, <span class="dt">task =</span> mtcars.task)</a></code></pre>
<pre><code>##      dunn 
## 0.1462919</code></pre>
<p>Moreover, some measures require a certain type of prediction. For example in binary classification in order to calculate the AUC (<a href="measures.md">auc</a>) â€“ the area under the ROC (receiver operating characteristic) curve â€“ we have to make sure that posterior probabilities are predicted. For more information on ROC analysis, see the section on <a href="roc_analysis.md">ROC analysis</a>.</p>
<pre class="sourceCode r" id="cb19"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.rpart"</span>, <span class="dt">predict.type =</span> <span class="st">"prob"</span>)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2">mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, <span class="dt">task =</span> sonar.task)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">task =</span> sonar.task)</a>
<a class="sourceLine" id="cb19-4" data-line-number="4"></a>
<a class="sourceLine" id="cb19-5" data-line-number="5"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> auc)</a></code></pre>
<pre><code>##       auc 
## 0.9224018</code></pre>
<p>Also bear in mind that many of the performance measures that are available for classification, e.g., the false positive rate (<a href="measures.md">fpr</a>), are only suitable for binary problems.</p>
</section></section><section id="access-a-performance-measure" class="level2"><h2>Access a performance measure</h2>
<p>Performance measures in [%mlr] are objects of class <a href="&amp;makeMeasure">Measure</a>. If you are interested in the properties or requirements of a single measure you can access it directly. See the help page of <a href="&amp;makeMeasure">Measure</a> for information on the individual slots.</p>
<pre class="sourceCode r" id="cb21"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1">## Mean misclassification error</a>
<a class="sourceLine" id="cb21-2" data-line-number="2"><span class="kw">str</span>(mmce)</a></code></pre>
<pre><code>## List of 10
##  $ id        : chr "mmce"
##  $ minimize  : logi TRUE
##  $ properties: chr [1:4] "classif" "classif.multi" "req.pred" "req.truth"
##  $ fun       :function (task, model, pred, feats, extra.args)  
##   ..- attr(*, "srcref")=Class 'srcref'  atomic [1:8] 464 9 466 3 9 3 26960 26962
##   .. .. ..- attr(*, "srcfile")=Classes 'srcfilealias', 'srcfile' &lt;environment: 0x55866287cc80&gt; 
##  $ extra.args: list()
##  $ best      : num 0
##  $ worst     : num 1
##  $ name      : chr "Mean misclassification error"
##  $ note      : chr "Defined as: mean(response != truth)"
##  $ aggr      :List of 4
##   ..$ id        : chr "test.mean"
##   ..$ name      : chr "Test mean"
##   ..$ fun       :function (task, perf.test, perf.train, measure, group, pred)  
##   .. ..- attr(*, "srcref")=Class 'srcref'  atomic [1:8] 43 9 43 83 9 83 19016 19016
##   .. .. .. ..- attr(*, "srcfile")=Classes 'srcfilealias', 'srcfile' &lt;environment: 0x558661d21938&gt; 
##   ..$ properties: chr "req.test"
##   ..- attr(*, "class")= chr "Aggregation"
##  - attr(*, "class")= chr "Measure"</code></pre>
</section><section id="binary-classification" class="level2"><h2>Binary classification</h2>
<p>For binary classification specialized techniques exist to analyze the performance.</p>
<section id="plot-performance-versus-threshold" class="level3"><h3>Plot performance versus threshold</h3>
<p>As you may recall (see the previous section on <a href="predict.md">making predictions</a>) in binary classification we can adjust the threshold used to map probabilities to class labels. Helpful in this regard is are the functions [&amp;generateThreshVsPerfData] and [&amp;plotThreshVsPerf], which generate and plot, respectively, the learner performance versus the threshold.</p>
<p>For more performance plots and automatic threshold tuning see <a href="roc_analysis.md">here</a>.</p>
<p>In the following example we consider the <a href="&amp;mlbench::Sonar">Sonar</a> data set and plot the false positive rate (<a href="measures.md">fpr</a>), the false negative rate (<a href="measures.md">fnr</a>) as well as the misclassification rate (<a href="measures.md">mmce</a>) for all possible threshold values.</p>
<pre class="sourceCode r" id="cb23"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">lrn =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.lda"</span>, <span class="dt">predict.type =</span> <span class="st">"prob"</span>)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">n =<span class="st"> </span><span class="kw"><a href="../../reference/getTaskSize.html">getTaskSize</a></span>(sonar.task)</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">mod =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn, <span class="dt">task =</span> sonar.task, <span class="dt">subset =</span> <span class="kw">seq</span>(<span class="dv">1</span>, n, <span class="dt">by =</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb23-4" data-line-number="4">pred =<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">task =</span> sonar.task, <span class="dt">subset =</span> <span class="kw">seq</span>(<span class="dv">2</span>, n, <span class="dt">by =</span> <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb23-5" data-line-number="5"></a>
<a class="sourceLine" id="cb23-6" data-line-number="6">## Performance for the default threshold 0.5</a>
<a class="sourceLine" id="cb23-7" data-line-number="7"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, fnr, mmce))</a></code></pre>
<pre><code>##       fpr       fnr      mmce 
## 0.2500000 0.3035714 0.2788462</code></pre>
<pre class="sourceCode r" id="cb25"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">## Plot false negative and positive rates as well as the error rate versus the threshold</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">d =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(pred, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, fnr, mmce))</a>
<a class="sourceLine" id="cb25-3" data-line-number="3"><span class="kw"><a href="../../reference/plotThreshVsPerf.html">plotThreshVsPerf</a></span>(d)</a></code></pre>
<p><img src="performance_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
<p>There is an experimental [%ggvis] plotting function [&amp;plotThreshVsPerfGGVIS] which performs similarly to [&amp;plotThreshVsPerf] but instead of creating facetted subplots to visualize multiple learners and/or multiple measures, one of them is mapped to an interactive sidebar which selects what to display.</p>
<pre class="sourceCode r" id="cb26"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw"><a href="../../reference/plotThreshVsPerfGGVIS.html">plotThreshVsPerfGGVIS</a></span>(d)</a></code></pre>
</section><section id="roc-measures" class="level3"><h3>ROC measures</h3>
<p>For binary classification a large number of specialized measures exist, which can be nicely formatted into one matrix, see for example the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic page on wikipedia</a>.</p>
<p>We can generate a similiar table with the [&amp;calculateROCMeasures] function.</p>
<pre class="sourceCode r" id="cb27"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">r =<span class="st"> </span><span class="kw"><a href="../../reference/calculateROCMeasures.html">calculateROCMeasures</a></span>(pred)</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">r</a></code></pre>
<pre><code>##     predicted
## true M         R                            
##    M 39        17        tpr: 0.7  fnr: 0.3 
##    R 12        36        fpr: 0.25 tnr: 0.75
##      ppv: 0.76 for: 0.32 lrp: 2.79 acc: 0.72
##      fdr: 0.24 npv: 0.68 lrm: 0.4  dor: 6.88
## 
## 
## Abbreviations:
## tpr - True positive rate (Sensitivity, Recall)
## fpr - False positive rate (Fall-out)
## fnr - False negative rate (Miss rate)
## tnr - True negative rate (Specificity)
## ppv - Positive predictive value (Precision)
## for - False omission rate
## lrp - Positive likelihood ratio (LR+)
## fdr - False discovery rate
## npv - Negative predictive value
## acc - Accuracy
## lrm - Negative likelihood ratio (LR-)
## dor - Diagnostic odds ratio</code></pre>
<p>The top left <span class="math inline">\(2 \times 2\)</span> matrix is the <a href="predict.md#confusion-matrix">confusion matrix</a>, which shows the relative frequency of correctly and incorrectly classified observations. Below and to the right a large number of performance measures that can be inferred from the confusion matrix are added. By default some additional info about the measures is printed. You can turn this off using the <code>abbreviations</code> argument of the <a href="&amp;calculateROCMeasures">print</a> method: <code>print(r, abbreviations = FALSE)</code>.</p>
</section></section>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#available-performance-measures">Available performance measures</a></li>
      <li><a href="#listing-measures">Listing measures</a></li>
      <li><a href="#calculate-performance-measures">Calculate performance measures</a></li>
      <li><a href="#access-a-performance-measure">Access a performance measure</a></li>
      <li><a href="#binary-classification">Binary classification</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio, Mason Gallo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
