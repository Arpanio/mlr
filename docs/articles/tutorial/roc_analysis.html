<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>ROC Analysis and Performance Curves • mlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../../index.html">mlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../../articles/mlr.html">Get Started</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/task.html">Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learners.html">Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/predict.html">Predict</a>
    </li>
    <li>
      <a href="../../articles/tutorial/performance.html">Performance</a>
    </li>
    <li>
      <a href="../../articles/tutorial/resample.html">Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/tune.html">Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/benchmark_experiments.html">Benchmark Experiments</a>
    </li>
    <li>
      <a href="../../articles/tutorial/parallelization.html">Parallelization</a>
    </li>
    <li>
      <a href="../../articles/tutorial/visualization.html">Visualization</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Advanced
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/configuring.html">Configuration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/wrapper.html">Wrapped Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/preproc.html">Preprocessing</a>
    </li>
    <li>
      <a href="../../articles/tutorial/impute.html">Imputation</a>
    </li>
    <li>
      <a href="../../articles/tutorial/bagging.html">Generic Bagging</a>
    </li>
    <li>
      <a href="../../articles/tutorial/advanced_tune.html">Advanced Tuning</a>
    </li>
    <li>
      <a href="../../articles/tutorial/feature_selection.html">Feature Selection</a>
    </li>
    <li>
      <a href="../../articles/tutorial/nested_resampling.html">Nested Resampling</a>
    </li>
    <li>
      <a href="../../articles/tutorial/cost_sensitive_classif.html">Cost-Sensitive Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/imbalanced_classification_problems.html">Imbalanced Classification Problems</a>
    </li>
    <li>
      <a href="../../articles/tutorial/roc_analysis.html">ROC Analysis and Performance Curves</a>
    </li>
    <li>
      <a href="../../articles/tutorial/multilabel.html">Multilabel Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial/learning_curve_analysis.html">Learning Curve Analysis</a>
    </li>
    <li>
      <a href="../../articles/tutorial/partial_dependence.html">Partial Dependence Plots</a>
    </li>
    <li>
      <a href="../../articles/tutorial/classifier_calibration.html">Classifier Calibration</a>
    </li>
    <li>
      <a href="../../articles/tutorial/hyperparameter_tuning_effects.html">Hyperparameter Tuning Effects</a>
    </li>
    <li>
      <a href="../../articles/tutorial/out_of_bag_predictions.html">Out-of-Bag Predictions</a>
    </li>
    <li>
      <a href="../../articles/tutorial/handling_of_spatial_data.html">Handling of Spatial Data</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Extend
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/custom_learners.html">Create Custom Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/custom_measures.html">Create Custom Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_imputation.html">Create Imputation Methods</a>
    </li>
    <li>
      <a href="../../articles/tutorial/create_filter.html">Create Custom Filters</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Appendix
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/tutorial/example_tasks.html">Example Tasks</a>
    </li>
    <li>
      <a href="../../articles/tutorial/integrated_learners.html">Integrated Learners</a>
    </li>
    <li>
      <a href="../../articles/tutorial/implemented_measures.html">Implemented Measures</a>
    </li>
    <li>
      <a href="../../articles/tutorial/filter_methods.html">Integrated Filter Methods</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/mlr-org/mlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form" role="search" method="GET" action="../../search.html">
        <div class="input-group">
          <input type="text" class="form-control" placeholder="Search" id="search" name="search">
</div>
        <button type="submit" class="btn btn-default">
          <i class="glyphicon glyphicon-search"></i>
        </button>
      </form>
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>ROC Analysis and Performance Curves</h1>
            
          </div>

    
    
<div class="contents">
<p>For binary scoring classifiers a <em>threshold</em> (or <em>cutoff</em>) value controls how predicted posterior probabilities are converted into class labels. ROC curves and other performance plots serve to visualize and analyse the relationship between one or two performance measures and the threshold.</p>
<p>This page is mainly devoted to <em>receiver operating characteristic</em> (ROC) curves that plot the <em>true positive rate</em> (sensitivity) on the vertical axis against the <em>false positive rate</em> (1 - specificity, fall-out) on the horizontal axis for all possible threshold values. Creating other performance plots like <em>lift charts</em> or <em>precision/recall graphs</em> works analogously and is shown briefly.</p>
<p>In addition to performance visualization ROC curves are helpful in</p>
<ul>
<li>determining an optimal decision threshold for given class prior probabilities and misclassification costs (for alternatives see also the pages about <a href="cost_sensitive_classif.md">cost-sensitive classification</a> and <a href="over_and_undersampling.md">imbalanced classification problems</a> in this tutorial),</li>
<li>identifying regions where one classifier outperforms another and building suitable multi-classifier systems,</li>
<li>obtaining calibrated estimates of the posterior probabilities.</li>
</ul>
<p>For more information see the tutorials and introductory papers by <a href="http://binf.gmu.edu/mmasso/ROC101.pdf">Fawcett (2004)</a>, <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">Fawcett (2006)</a> as well as <a href="http://www.cs.bris.ac.uk/~flach/ICML04tutorial/index.html">Flach (ICML 2004)</a>.</p>
<p>In many applications as, e.g., diagnostic tests or spam detection, there is uncertainty about the class priors or the misclassification costs at the time of prediction, for example because it’s hard to quantify the costs or because costs and class priors vary over time. Under these circumstances the classifier is expected to work well for a whole range of decision thresholds and the area under the ROC curve (AUC) provides a scalar performance measure for comparing and selecting classifiers. [%mlr] provides the AUC for binary classification (<a href="measures.md">auc</a>) and also several generalizations of the AUC to the multi-class case (e.g., <a href="measures.md">multiclass.au1p</a>, <a href="measures.md">multiclass.au1u</a> based on <a href="https://www.math.ucdavis.edu/~saito/data/roc/ferri-class-perf-metrics.pdf">Ferri et al. (2009)</a>).</p>
<p>[%mlr] offers three ways to plot ROC and other performance curves.</p>
<ol style="list-style-type: decimal">
<li>Function [&amp;plotROCCurves] can, based on the output of [&amp;generateThreshVsPerfData], plot performance curves for any pair of <a href="measures.md">performance measures</a> available in [%mlr].</li>
<li>[%mlr] offers an interface to package [%ROCR] through function [&amp;asROCRPrediction].</li>
<li>[%mlr]’s function [&amp;plotViperCharts] provides an interface to <a href="http://viper.ijs.si">ViperCharts</a>.</li>
</ol>
<p>With [%mlr] version 2.8 functions <code>generateROCRCurvesData</code>, <code>plotROCRCurves</code>, and <code>plotROCRCurvesGGVIS</code> were deprecated.</p>
<p>Below are some examples that demonstrate the three possible ways. Note that you can only use <a href="learner.md">learners</a> that are capable of predicting probabilities. Have a look at the <a href="integrated_learners.md">learner table in the Appendix</a> or run <code><a href="../../reference/listLearners.html">listLearners("classif", properties = c("twoclass", "prob"))</a></code> to get a list of all learners that support this.</p>
<div id="performance-plots-with-plotroccurves" class="section level2">
<h2 class="hasAnchor">
<a href="#performance-plots-with-plotroccurves" class="anchor"></a>Performance plots with plotROCCurves</h2>
<p>As you might recall [&amp;generateThreshVsPerfData] calculates one or several performance measures for a sequence of decision thresholds from 0 to 1. It provides S3 methods for objects of class [&amp;Prediction], [&amp;ResampleResult] and [&amp;BenchmarkResult] (resulting from <a href="&amp;predict.WrappedModel">predict</a>, [&amp;resample] or [&amp;benchmark]). [&amp;plotROCCurves] plots the result of [&amp;generateThreshVsPerfData] using [%ggplot2].</p>
<div id="example-1-single-predictions" class="section level3">
<h3 class="hasAnchor">
<a href="#example-1-single-predictions" class="anchor"></a>Example 1: Single predictions</h3>
<p>We consider the <a href="&amp;mlbench::Sonar">Sonar</a> data set from package [%mlbench], which poses a binary classification problem ([&amp;sonar.task]) and apply <a href="&amp;MASS::lda">linear discriminant analysis</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="kw"><a href="../../reference/getTaskSize.html">getTaskSize</a></span>(sonar.task)
train.set =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dt">size =</span> <span class="kw">round</span>(<span class="dv">2</span>/<span class="dv">3</span> *<span class="st"> </span>n))
test.set =<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">seq_len</span>(n), train.set)

lrn1 =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.lda"</span>, <span class="dt">predict.type =</span> <span class="st">"prob"</span>)
mod1 =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn1, sonar.task, <span class="dt">subset =</span> train.set)
pred1 =<span class="st"> </span><span class="kw">predict</span>(mod1, <span class="dt">task =</span> sonar.task, <span class="dt">subset =</span> test.set)</code></pre></div>
<p>Since we want to plot ROC curves we calculate the false and true positive rates (<a href="measures.md">fpr</a> and <a href="measures.md">tpr</a>). Additionally, we also compute error rates (<a href="measures.md">mmce</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(pred1, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, tpr, mmce))</code></pre></div>
<p>[&amp;generateThreshVsPerfData] returns an object of class <a href="&amp;generateThreshVsPerfData">ThreshVsPerfData</a> which contains the performance values in the <code>$data</code> element.</p>
<p>Per default, [&amp;plotROCCurves] plots the performance values of the first two measures passed to [&amp;generateThreshVsPerfData]. The first is shown on the x-axis, the second on the y-axis. Moreover, a diagonal line that represents the performance of a random classifier is added. You can remove the diagonal by setting <code>diagonal = FALSE</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-4-1.png" width="75%"></p>
<p>The corresponding area under curve (<a href="measures.md">auc</a>) can be calculated as usual by calling [&amp;performance].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred1, auc)</code></pre></div>
<pre><code>##       auc 
## 0.8145299</code></pre>
<p>[&amp;plotROCCurves] always requires a pair of performance measures that are plotted against each other. If you want to plot individual measures versus the decision threshold you can use function [&amp;plotThreshVsPerf].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotThreshVsPerf.html">plotThreshVsPerf</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-6-1.png" width="672"></p>
<p>Additional to <a href="&amp;MASS::lda">linear discriminant analysis</a> we try a support vector machine with RBF kernel (<a href="&amp;kernlab::ksvm">ksvm</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrn2 =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.ksvm"</span>, <span class="dt">predict.type =</span> <span class="st">"prob"</span>)
mod2 =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn2, sonar.task, <span class="dt">subset =</span> train.set)
pred2 =<span class="st"> </span><span class="kw">predict</span>(mod2, <span class="dt">task =</span> sonar.task, <span class="dt">subset =</span> test.set)</code></pre></div>
<p>In order to compare the performance of the two learners you might want to display the two corresponding ROC curves in one plot. For this purpose just pass a named <a href="&amp;base::list">list</a> of [&amp;Prediction]s to [&amp;generateThreshVsPerfData].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(<span class="kw">list</span>(<span class="dt">lda =</span> pred1, <span class="dt">ksvm =</span> pred2), <span class="dt">measures =</span> <span class="kw">list</span>(fpr, tpr))
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-8-1.png" width="672"></p>
<p>It’s clear from the plot above that <a href="&amp;kernlab::ksvm">ksvm</a> has a slightly higher AUC than <a href="&amp;MASS::lda">lda</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/performance.html">performance</a></span>(pred2, auc)</code></pre></div>
<pre><code>##       auc 
## 0.9188034</code></pre>
<p>Based on the <code>$data</code> member of <code>df</code> you can easily generate custom plots. Below the curves for the two learners are superposed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(<span class="dt">x =</span> fpr, <span class="dt">y =</span> tpr, <span class="dt">color =</span> learner, <span class="dt">data =</span> df$data, <span class="dt">geom =</span> <span class="st">"path"</span>)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-10-1.png" width="75%"></p>
<p>It is easily possible to generate other performance plots by passing the appropriate performance measures to [&amp;generateThreshVsPerfData] and [&amp;plotROCCurves]. Below, we generate a <em>precision/recall graph</em> (precision = positive predictive value = ppv, recall = tpr) and a <em>sensitivity/specificity plot</em> (sensitivity = tpr, specificity = tnr).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(<span class="kw">list</span>(<span class="dt">lda =</span> pred1, <span class="dt">ksvm =</span> pred2), <span class="dt">measures =</span> <span class="kw">list</span>(ppv, tpr, tnr))

## Precision/recall graph
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df, <span class="dt">measures =</span> <span class="kw">list</span>(tpr, ppv), <span class="dt">diagonal =</span> <span class="ot">FALSE</span>)

## Sensitivity/specificity plot
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df, <span class="dt">measures =</span> <span class="kw">list</span>(tnr, tpr), <span class="dt">diagonal =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-11-1.png" width="50%"><img src="roc_analysis_files/figure-html/unnamed-chunk-11-2.png" width="50%"></p>
</div>
<div id="example-2-benchmark-experiment" class="section level3">
<h3 class="hasAnchor">
<a href="#example-2-benchmark-experiment" class="anchor"></a>Example 2: Benchmark experiment</h3>
<p>The analysis in the example above can be improved a little. Instead of writing individual code for training/prediction of each learner, which can become tedious very quickly, we can use function [&amp;benchmark] (see also <a href="benchmark_experiments.md">Benchmark Experiments</a>) and, ideally, the support vector machine should have been <a href="tune.md">tuned</a>.</p>
<p>We again consider the <a href="&amp;mlbench::Sonar">Sonar</a> data set and apply <a href="&amp;MASS::lda">lda</a> as well as <a href="&amp;kernlab::ksvm">ksvm</a>. We first generate a <a href="&amp;makeTuneWrapper">tuning wrapper</a> for <a href="&amp;kernlab::ksvm">ksvm</a>. The cost parameter is tuned on a (for demonstration purposes small) parameter grid. We assume that we are interested in a good performance over the complete threshold range and therefore tune with regard to the <a href="measures.md">auc</a>. The error rate (<a href="measures.md">mmce</a>) for a threshold value of 0.5 is reported as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Tune wrapper for ksvm
rdesc.inner =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"Holdout"</span>)
ms =<span class="st"> </span><span class="kw">list</span>(auc, mmce)
ps =<span class="st"> </span><span class="kw">makeParamSet</span>(
  <span class="kw">makeDiscreteParam</span>(<span class="st">"C"</span>, <span class="dv">2</span>^(-<span class="dv">1</span>:<span class="dv">1</span>))
)
ctrl =<span class="st"> </span><span class="kw"><a href="../../reference/makeTuneControlGrid.html">makeTuneControlGrid</a></span>()
lrn2 =<span class="st"> </span><span class="kw"><a href="../../reference/makeTuneWrapper.html">makeTuneWrapper</a></span>(lrn2, rdesc.inner, ms, ps, ctrl, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>Below the actual benchmark experiment is conducted. As resampling strategy we use 5-fold cross-validation and again calculate the <a href="measures.md">auc</a> as well as the error rate (for a threshold/cutoff value of 0.5).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Benchmark experiment
lrns =<span class="st"> </span><span class="kw">list</span>(lrn1, lrn2)
rdesc.outer =<span class="st"> </span><span class="kw"><a href="../../reference/makeResampleDesc.html">makeResampleDesc</a></span>(<span class="st">"CV"</span>, <span class="dt">iters =</span> <span class="dv">5</span>)

bmr =<span class="st"> </span><span class="kw"><a href="../../reference/benchmark.html">benchmark</a></span>(lrns, <span class="dt">tasks =</span> sonar.task, <span class="dt">resampling =</span> rdesc.outer, <span class="dt">measures =</span> ms, <span class="dt">show.info =</span> <span class="ot">FALSE</span>)
bmr</code></pre></div>
<pre><code>##         task.id         learner.id auc.test.mean mmce.test.mean
## 1 Sonar-example        classif.lda     0.8116447      0.2448316
## 2 Sonar-example classif.ksvm.tuned     0.9201627      0.1635308</code></pre>
<p>Calling [&amp;generateThreshVsPerfData] and [&amp;plotROCCurves] on the <a href="&amp;BenchmarkResult">benchmark result</a> produces a plot with ROC curves for all learners in the experiment.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(bmr, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, tpr, mmce))
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-14-1.png" width="672"></p>
<p>Per default, [&amp;generateThreshVsPerfData] calculates aggregated performances according to the chosen resampling strategy (5-fold cross-validation) and aggregation scheme (<a href="&amp;aggregations">test.mean</a>) for each threshold in the sequence. This way we get <em>threshold-averaged</em> ROC curves.</p>
<p>If you want to plot the individual ROC curves for each resample iteration set <code>aggregate = FALSE</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(bmr, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, tpr, mmce), <span class="dt">aggregate =</span> <span class="ot">FALSE</span>)
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-15-1.png" width="672"></p>
<p>The same applies for [&amp;plotThreshVsPerf].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../../reference/plotThreshVsPerf.html">plotThreshVsPerf</a></span>(df) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">strip.text.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">7</span>))</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-16-1.png" width="672"></p>
<p>An alternative to averaging is to just merge the 5 test folds and draw a single ROC curve. Merging can be achieved by manually changing the <a href="&amp;base::class">class</a> attribute of the prediction objects from [&amp;ResamplePrediction] to [&amp;Prediction].</p>
<p>Below, the predictions are extracted from the [&amp;BenchmarkResult] via function [&amp;getBMRPredictions], the <a href="&amp;base::class">class</a> is changed and the ROC curves are created.</p>
<p>Averaging methods are normally preferred (cp. <a href="https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf">Fawcett, 2006</a>), as they permit to assess the variability, which is needed to properly compare classifier performance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Extract predictions
preds =<span class="st"> </span><span class="kw"><a href="../../reference/getBMRPredictions.html">getBMRPredictions</a></span>(bmr, <span class="dt">drop =</span> <span class="ot">TRUE</span>)

## Change the class attribute
preds2 =<span class="st"> </span><span class="kw">lapply</span>(preds, function(x) {<span class="kw">class</span>(x) =<span class="st"> "Prediction"</span>; <span class="kw">return</span>(x)})

## Draw ROC curves
df =<span class="st"> </span><span class="kw"><a href="../../reference/generateThreshVsPerfData.html">generateThreshVsPerfData</a></span>(preds2, <span class="dt">measures =</span> <span class="kw">list</span>(fpr, tpr, mmce))
<span class="kw"><a href="../../reference/plotROCCurves.html">plotROCCurves</a></span>(df)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-17-1.png" width="672"></p>
<p>Again, you can easily create other standard evaluation plots by passing the appropriate performance measures to [&amp;generateThreshVsPerfData] and [&amp;plotROCCurves].</p>
</div>
</div>
<div id="performance-plots-with-asrocrprediction" class="section level2">
<h2 class="hasAnchor">
<a href="#performance-plots-with-asrocrprediction" class="anchor"></a>Performance plots with asROCRPrediction</h2>
<p>Drawing performance plots with package [%ROCR] works through three basic commands:</p>
<ol style="list-style-type: decimal">
<li>[&amp;ROCR::prediction]: Create a [%ROCR] [prediction](&amp;ROCR::prediction-class) object.</li>
<li>given prediction object.</li>
<li>
<a href="&amp;ROCR::plot-methods">ROCR::plot</a>: Generate the performance plot.</li>
</ol>
<p>[%mlr]’s function [&amp;asROCRPrediction] converts an [%mlr] [&amp;Prediction] object to a [%ROCR] [prediction](&amp;ROCR::prediction-class) object, so you can easily generate performance plots by doing steps 2. and 3. yourself. [%ROCR]’s <a href="&amp;ROCR::plot-methods">plot</a> method has some nice features which are not (yet) available in [&amp;plotROCCurves], for example plotting the convex hull of the ROC curves. Some examples are shown below.</p>
<div id="example-1-single-predictions-continued" class="section level3">
<h3 class="hasAnchor">
<a href="#example-1-single-predictions-continued" class="anchor"></a>Example 1: Single predictions (continued)</h3>
<p>We go back to out first example where we trained and predicted <a href="&amp;MASS::lda">lda</a> on the <a href="&amp;sonar.task">sonar classification task</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="kw"><a href="../../reference/getTaskSize.html">getTaskSize</a></span>(sonar.task)
train.set =<span class="st"> </span><span class="kw">sample</span>(n, <span class="dt">size =</span> <span class="kw">round</span>(<span class="dv">2</span>/<span class="dv">3</span> *<span class="st"> </span>n))
test.set =<span class="st"> </span><span class="kw">setdiff</span>(<span class="kw">seq_len</span>(n), train.set)

## Train and predict linear discriminant analysis
lrn1 =<span class="st"> </span><span class="kw"><a href="../../reference/makeLearner.html">makeLearner</a></span>(<span class="st">"classif.lda"</span>, <span class="dt">predict.type =</span> <span class="st">"prob"</span>)
mod1 =<span class="st"> </span><span class="kw"><a href="../../reference/train.html">train</a></span>(lrn1, sonar.task, <span class="dt">subset =</span> train.set)
pred1 =<span class="st"> </span><span class="kw">predict</span>(mod1, <span class="dt">task =</span> sonar.task, <span class="dt">subset =</span> test.set)</code></pre></div>
<p>Below we use [&amp;asROCRPrediction] to convert the lda prediction, let [%ROCR] calculate the true and false positive rate and plot the ROC curve.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Convert prediction
ROCRpred1 =<span class="st"> </span><span class="kw"><a href="../../reference/asROCRPrediction.html">asROCRPrediction</a></span>(pred1)

## Calculate true and false positive rate
ROCRperf1 =<span class="st"> </span>ROCR::<span class="kw"><a href="../../reference/performance.html">performance</a></span>(ROCRpred1, <span class="st">"tpr"</span>, <span class="st">"fpr"</span>)

## Draw ROC curve
ROCR::<span class="kw">plot</span>(ROCRperf1)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-19-1.png" width="75%"></p>
<p>Below is the same ROC curve, but we make use of some more graphical parameters: The ROC curve is color-coded by the threshold and selected threshold values are printed on the curve. Additionally, the convex hull (black broken line) of the ROC curve is drawn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Draw ROC curve
ROCR::<span class="kw">plot</span>(ROCRperf1, <span class="dt">colorize =</span> <span class="ot">TRUE</span>, <span class="dt">print.cutoffs.at =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>), <span class="dt">lwd =</span> <span class="dv">2</span>)

## Draw convex hull of ROC curve
ch =<span class="st"> </span>ROCR::<span class="kw"><a href="../../reference/performance.html">performance</a></span>(ROCRpred1, <span class="st">"rch"</span>)
ROCR::<span class="kw">plot</span>(ch, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-20-1.png" width="75%"></p>
</div>
<div id="example-2-benchmark-experiments-continued" class="section level3">
<h3 class="hasAnchor">
<a href="#example-2-benchmark-experiments-continued" class="anchor"></a>Example 2: Benchmark experiments (continued)</h3>
<p>We again consider the benchmark experiment conducted earlier. We first extract the predictions by [&amp;getBMRPredictions] and then convert them via function [&amp;asROCRPrediction].</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Extract predictions
preds =<span class="st"> </span><span class="kw"><a href="../../reference/getBMRPredictions.html">getBMRPredictions</a></span>(bmr, <span class="dt">drop =</span> <span class="ot">TRUE</span>)

## Convert predictions
ROCRpreds =<span class="st"> </span><span class="kw">lapply</span>(preds, asROCRPrediction)

## Calculate true and false positive rate
ROCRperfs =<span class="st"> </span><span class="kw">lapply</span>(ROCRpreds, function(x) ROCR::<span class="kw"><a href="../../reference/performance.html">performance</a></span>(x, <span class="st">"tpr"</span>, <span class="st">"fpr"</span>))</code></pre></div>
<p>We draw the vertically averaged ROC curves (solid lines) as well as the ROC curves for the individual resampling iterations (broken lines). Moreover, standard error bars are plotted for selected true positive rates (0.1, 0.2, …, 0.9). See [%ROCR]’s <a href="&amp;ROCR::plot-methods">plot</a> function for details.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## lda average ROC curve
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">1</span>]], <span class="dt">col =</span> <span class="st">"blue"</span>, <span class="dt">avg =</span> <span class="st">"vertical"</span>, <span class="dt">spread.estimate =</span> <span class="st">"stderror"</span>,
  <span class="dt">show.spread.at =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.1</span>), <span class="dt">plotCI.col =</span> <span class="st">"blue"</span>, <span class="dt">plotCI.lwd =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)
## lda individual ROC curves
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">1</span>]], <span class="dt">col =</span> <span class="st">"blue"</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="fl">0.25</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)

## ksvm average ROC curve
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">2</span>]], <span class="dt">col =</span> <span class="st">"red"</span>, <span class="dt">avg =</span> <span class="st">"vertical"</span>, <span class="dt">spread.estimate =</span> <span class="st">"stderror"</span>,
  <span class="dt">show.spread.at =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="fl">0.6</span>, <span class="fl">0.1</span>), <span class="dt">plotCI.col =</span> <span class="st">"red"</span>, <span class="dt">plotCI.lwd =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
## ksvm individual ROC curves
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">2</span>]], <span class="dt">col =</span> <span class="st">"red"</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="fl">0.25</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)

<span class="kw">legend</span>(<span class="st">"bottomright"</span>, <span class="dt">legend =</span> <span class="kw"><a href="../../reference/getBMRLearnerIds.html">getBMRLearnerIds</a></span>(bmr), <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>))</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-22-1.png" width="75%"></p>
<p>In order to create other evaluation plots like <em>precision/recall graphs</em> you just have to change the performance measures when calling <a href="Calculate%20one%20or%20more%20performance%20measures%20for%20the">&amp;ROCR::performance</a>. (Note that you have to use the measures provided by [%ROCR] listed <a href="&amp;ROCR::performance">here</a> and not [%mlr]’s performance measures.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Extract and convert predictions
preds =<span class="st"> </span><span class="kw"><a href="../../reference/getBMRPredictions.html">getBMRPredictions</a></span>(bmr, <span class="dt">drop =</span> <span class="ot">TRUE</span>)
ROCRpreds =<span class="st"> </span><span class="kw">lapply</span>(preds, asROCRPrediction)

## Calculate precision and recall
ROCRperfs =<span class="st"> </span><span class="kw">lapply</span>(ROCRpreds, function(x) ROCR::<span class="kw"><a href="../../reference/performance.html">performance</a></span>(x, <span class="st">"prec"</span>, <span class="st">"rec"</span>))

## Draw performance plot
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">1</span>]], <span class="dt">col =</span> <span class="st">"blue"</span>, <span class="dt">avg =</span> <span class="st">"threshold"</span>)
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">2</span>]], <span class="dt">col =</span> <span class="st">"red"</span>, <span class="dt">avg =</span> <span class="st">"threshold"</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
<span class="kw">legend</span>(<span class="st">"bottomleft"</span>, <span class="dt">legend =</span> <span class="kw"><a href="../../reference/getBMRLearnerIds.html">getBMRLearnerIds</a></span>(bmr), <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>))</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-23-1.png" width="75%"></p>
<p>If you want to plot a performance measure versus the threshold, specify only one measure when calling <a href="Calculate%20one%20or%20more%20performance%20measures%20for%20the">&amp;ROCR::performance</a>. Below the average accuracy over the 5 cross-validation iterations is plotted against the threshold. Moreover, boxplots for certain threshold values (0.1, 0.2, …, 0.9) are drawn.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Extract and convert predictions
preds =<span class="st"> </span><span class="kw"><a href="../../reference/getBMRPredictions.html">getBMRPredictions</a></span>(bmr, <span class="dt">drop =</span> <span class="ot">TRUE</span>)
ROCRpreds =<span class="st"> </span><span class="kw">lapply</span>(preds, asROCRPrediction)

## Calculate accuracy
ROCRperfs =<span class="st"> </span><span class="kw">lapply</span>(ROCRpreds, function(x) ROCR::<span class="kw"><a href="../../reference/performance.html">performance</a></span>(x, <span class="st">"acc"</span>))

## Plot accuracy versus threshold
<span class="kw">plot</span>(ROCRperfs[[<span class="dv">1</span>]], <span class="dt">avg =</span> <span class="st">"vertical"</span>, <span class="dt">spread.estimate =</span> <span class="st">"boxplot"</span>, <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">col =</span> <span class="st">"blue"</span>,
  <span class="dt">show.spread.at =</span> <span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.1</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">xlab =</span> <span class="st">"Threshold"</span>)</code></pre></div>
<p><img src="roc_analysis_files/figure-html/unnamed-chunk-24-1.png" width="75%"></p>
</div>
</div>
<div id="viper-charts" class="section level2">
<h2 class="hasAnchor">
<a href="#viper-charts" class="anchor"></a>Viper charts</h2>
<p>[%mlr] also supports <a href="http://viper.ijs.si/">ViperCharts</a> for plotting ROC and other performance curves. Like [&amp;generateThreshVsPerfData] it has S3 methods for objects of class [&amp;Prediction], [&amp;ResampleResult] and [&amp;BenchmarkResult]. Below plots for the benchmark experiment (Example 2) are generated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z =<span class="st"> </span><span class="kw"><a href="../../reference/plotViperCharts.html">plotViperCharts</a></span>(bmr, <span class="dt">chart =</span> <span class="st">"rocc"</span>, <span class="dt">browse =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>You can see the plot created this way <a href="http://viper.ijs.si/chart/roc/a307712e-31b3-4a99-a919-7f14a660ad23/">here</a>. Note that besides ROC curves you get several other plots like lift charts or cost curves. For details, see [&amp;plotViperCharts].</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#performance-plots-with-plotroccurves">Performance plots with plotROCCurves</a></li>
      <li><a href="#performance-plots-with-asrocrprediction">Performance plots with asROCRPrediction</a></li>
      <li><a href="#viper-charts">Viper charts</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Bernd Bischl, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Zachary Jones, Giuseppe Casalicchio, Mason Gallo.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
